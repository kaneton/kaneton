%%
%% licence       kaneton licence
%%
%% project       kaneton
%%
%% file          /home/mycure/kaneton/view/lectures/arch-mips/arch-mips.tex
%%
%% created       julien quintard   [wed nov 23 22:07:25 2005]
%% updated       julien quintard   [thu jun 29 09:17:33 2006]
%%

%
% template
%

\input{../../templates/lecture.tex}

%
% title
%

\title{MIPS Architecture}

%
% authors
%

\author
{
  Julien~Quintard\inst{1}
}

%
% figures
%

\pgfdeclareimage[interpolate=true,width=188pt,height=97pt]
                {philosophies}
		{figures/philosophies}

\pgfdeclareimage[interpolate=true,width=166pt,height=26pt]
                {r-format}
		{figures/r-format}
\pgfdeclareimage[interpolate=true,width=166pt,height=26pt]
                {i-format}
		{figures/i-format}
\pgfdeclareimage[interpolate=true,width=166pt,height=26pt]
                {j-format}
		{figures/j-format}

\pgfdeclareimage[interpolate=true,width=166pt,height=46pt]
                {overflow-handling}
		{figures/overflow-handling}
\pgfdeclareimage[interpolate=true,width=166pt,height=46pt]
                {overflow-avoiding}
		{figures/overflow-avoiding}

\pgfdeclareimage[interpolate=true,width=270pt,height=129pt]
                {endianness}
		{figures/endianness}
\pgfdeclareimage[interpolate=true,width=82pt,height=129pt]
                {memory-layout}
		{figures/memory-layout}
\pgfdeclareimage[interpolate=true,width=270pt,height=129pt]
                {memory-access}
		{figures/memory-access}

\pgfdeclareimage[interpolate=true,width=178pt,height=79pt]
                {jump-instruction}
		{figures/jump-instruction}

\pgfdeclareimage[interpolate=true,width=163pt,height=45pt]
                {amdhal-rule}
		{figures/amdhal-rule}

\pgfdeclareimage[interpolate=true,width=163pt,height=92pt]
                {pipeline-overview}
		{figures/pipeline-overview}
\pgfdeclareimage[interpolate=true,width=196pt,height=90pt]
                {pipeline-balancing}
		{figures/pipeline-balancing}
\pgfdeclareimage[interpolate=true,width=172pt,height=105pt]
                {moore-law}
		{figures/moore-law}
\pgfdeclareimage[interpolate=true,width=210pt,height=120pt]
                {pipeline-views}
		{figures/pipeline-views}
\pgfdeclareimage[interpolate=true,width=128pt,height=150pt]
                {detailed-pipeline}
		{figures/detailed-pipeline}
\pgfdeclareimage[interpolate=true,width=56pt,height=68pt]
                {next-instruction-stream}
		{figures/next-instruction-stream}
\pgfdeclareimage[interpolate=true,width=66pt,height=134pt]
                {decode-branch}
		{figures/decode-branch}
\pgfdeclareimage[interpolate=true,width=247pt,height=91pt]
                {decode-multiplexer}
		{figures/decode-multiplexer}
\pgfdeclareimage[interpolate=true,width=157pt,height=86pt]
                {comparison-decode-execute-1}
		{figures/comparison-decode-execute-1}
\pgfdeclareimage[interpolate=true,width=139pt,height=116pt]
                {comparison-decode-execute-2}
		{figures/comparison-decode-execute-2}
\pgfdeclareimage[interpolate=true,width=90pt,height=101pt]
                {comparison-decode-execute-3}
		{figures/comparison-decode-execute-3}
\pgfdeclareimage[interpolate=true,width=75pt,height=144pt]
                {decode-stage}
		{figures/decode-stage}

\pgfdeclareimage[interpolate=true,width=133pt,height=61pt]
                {dependency-problem-overview}
		{figures/dependency-problem-overview}
\pgfdeclareimage[interpolate=true,width=121pt,height=61pt]
                {dependency-solution-overview}
		{figures/dependency-solution-overview}
\pgfdeclareimage[interpolate=true,width=155pt,height=151pt]
                {dependency-detailed-pipeline}
		{figures/dependency-detailed-pipeline}
\pgfdeclareimage[interpolate=true,width=161pt,height=81pt]
                {bypasses}
		{figures/bypasses}
\pgfdeclareimage[interpolate=true,width=298pt,height=141pt]
                {example-pipeline}
		{figures/example-pipeline}
\pgfdeclareimage[interpolate=true,width=258pt,height=150pt]
                {strlen-part-1}
		{figures/strlen-part-1}
\pgfdeclareimage[interpolate=true,width=208pt,height=139pt]
                {strlen-part-2}
		{figures/strlen-part-2}
\pgfdeclareimage[interpolate=true,width=218pt,height=128pt]
                {strlen-part-3}
		{figures/strlen-part-3}
\pgfdeclareimage[interpolate=true,width=218pt,height=108pt]
                {strlen-part-4}
		{figures/strlen-part-4}

\pgfdeclareimage[interpolate=true,width=175pt,height=103pt]
                {software-pipeline}
		{figures/software-pipeline}

\pgfdeclareimage[interpolate=true,width=267pt,height=135pt]
                {superpipeline-views}
		{figures/superpipeline-views}
\pgfdeclareimage[interpolate=true,width=205pt,height=173pt]
                {detailed-superpipeline}
		{figures/detailed-superpipeline}
\pgfdeclareimage[interpolate=true,width=221pt,height=101pt]
                {superpipeline-bypasses}
		{figures/superpipeline-bypasses}
\pgfdeclareimage[interpolate=true,width=101pt,height=41pt]
                {multiplication-pipeline}
		{figures/multiplication-pipeline}
\pgfdeclareimage[interpolate=true,width=210pt,height=46pt]
                {multiplication-dependency}
		{figures/multiplication-dependency}
\pgfdeclareimage[interpolate=true,width=101pt,height=41pt]
                {superscalar-pipeline}
		{figures/superscalar-pipeline}
\pgfdeclareimage[interpolate=true,width=163pt,height=137pt]
                {superscalar-example}
		{figures/superscalar-example}
\pgfdeclareimage[interpolate=true,width=157pt,height=152pt]
                {superscalar-correction}
		{figures/superscalar-correction}

\pgfdeclareimage[interpolate=true,width=101pt,height=61pt]
                {pibus-overview}
		{figures/pibus-overview}
\pgfdeclareimage[interpolate=true,width=218pt,height=121pt]
                {pibus-communication}
		{figures/pibus-communication}
\pgfdeclareimage[interpolate=true,width=185pt,height=130pt]
                {pibus-example}
		{figures/pibus-example}
\pgfdeclareimage[interpolate=true,width=251pt,height=125pt]
                {memory-organisation}
		{figures/memory-organisation}
\pgfdeclareimage[interpolate=true,width=60pt,height=60pt]
                {memory-access-time}
		{figures/memory-access-time}
\pgfdeclareimage[interpolate=true,width=120pt,height=102pt]
                {spatial-locality}
		{figures/spatial-locality}
\pgfdeclareimage[interpolate=true,width=251pt,height=121pt]
                {cache-hierarchy}
		{figures/cache-hierarchy}
\pgfdeclareimage[interpolate=true,width=98pt,height=112pt]
                {cache-overview}
		{figures/cache-overview}
\pgfdeclareimage[interpolate=true,width=208pt,height=111pt]
                {pibus-cache}
		{figures/pibus-cache}
\pgfdeclareimage[interpolate=true,width=128pt,height=93pt]
                {natural-partitionning}
		{figures/natural-partitionning}
\pgfdeclareimage[interpolate=true,width=147pt,height=111pt]
                {cache-example}
		{figures/cache-example}
\pgfdeclareimage[interpolate=true,width=162pt,height=127pt]
                {cache-internals}
		{figures/cache-internals}
\pgfdeclareimage[interpolate=true,width=171pt,height=186pt]
                {set-associative-overview}
		{figures/set-associative-overview}
\pgfdeclareimage[interpolate=true,width=152pt,height=147pt]
                {set-associative-internals}
		{figures/set-associative-internals}
\pgfdeclareimage[interpolate=true,width=121pt,height=80pt]
                {temporal-locality}
		{figures/temporal-locality}
\pgfdeclareimage[interpolate=true,width=221pt,height=151pt]
                {snoopy}
		{figures/snoopy}
\pgfdeclareimage[interpolate=true,width=173pt,height=148pt]
                {cache-automata}
		{figures/cache-automata}
\pgfdeclareimage[interpolate=true,width=141pt,height=30pt]
                {exercise-bus}
		{figures/exercise-bus}

%
% document
%

\begin{document}

%
% title frame
%

\begin{frame}
  \titlepage

  \begin{center}
    \logos
  \end{center}
\end{frame}

%
% outline frame
%

\begin{frame}
  \frametitle{Outline}
  \tableofcontents
\end{frame}

%
% introduction
%

\section{Introduction}

% 1)

\begin{frame}
  \frametitle{Description}

  \begin{itemize}[<+->]
    \item
      About \textbf{thirty} hours course.
    \item
      Concluded by an exam.
  \end{itemize}
\end{frame}

% 2)

\begin{frame}
  \frametitle{Contents}

  \begin{itemize}[<+->]
    \item
      External architecture.
    \item
      Pipeline.
    \item
      Compiler optimisations.
    \item
      Memory.
  \end{itemize}
\end{frame}

%
% overview
%

\section{Overview}

% 1)

\begin{frame}
  \frametitle{Overview}

  MIPS, for \textbf{M}icroprocessor without \textbf{I}nterlocked
  \textbf{P}ipeline \textbf{S}tages is a RISC microprocessor
  architecture developed by MIPS Computer Systems Inc.

  \nl

  The MIPS architecture studied in this course is the MIPS R3000.
\end{frame}

% 2)

\begin{frame}
  \frametitle{History}

  \begin{itemize}[<+->]
    \item
      \textbf{1975}: IBM invented the pipelining technique but seemed
      difficult to implement due to interlocks for complex instructions like
      multiplication and division.
    \item
      \textbf{1981}: a team led by John L. Hennessy at Stanford University
      started to build a RISC processor without interlocks.
    \item
      \textbf{1984}: John L. Hennessy left Stanford University to form
      the MIPS Computer Systems.
    \item
      \textbf{1985}: The first MIPS processor, the R2000, was released.
    \item
      \textbf{1988}: The R3000 is released and widely used by companies like
      Silicon Graphics.
  \end{itemize}
\end{frame}

% 3)

\begin{frame}
  \frametitle{History}

  \begin{itemize}[<+->]
    \item
      \textbf{1991}: The R4000 is released, a 64-bit microprocessor.
    \item
      \textbf{1992}: SGI decided to buy the MIPS company outright in order
      to guarantee the design would not be lost given the financial
      difficulties MIPS had while bringing it to market. The company became
      known as MIPS Technologies.
    \item
      \textbf{1994}: The R8000 is released, the first superscalar
      MIPS microprocessor.
    \item
      \textbf{1997}: 48-millionth MIPS-based CPU shipped, making it the
      first RISC CPU to ouship the famous Motorola 68000 family.
    \item
      \textbf{1999}: MIPS formalised their licensing system around two
      basic designs, the 32-bit MIPS32 and 64-bit MIPS64. Soon, NEC, Toshiba,
      Philips, LSI etc.. obtained licenses for the MIPS64.
  \end{itemize}
\end{frame}

% 4)

\begin{frame}
  \frametitle{Introduction}

  An architecture is composed of:

  \begin{itemize}[<+->]
    \item
      Visible registers.
    \item
      Instructions set.
    \item
      Addressing.
    \item
      Interrupts/exceptions system.
  \end{itemize}
\end{frame}

% 5)

\begin{frame}
  \frametitle{Description}

  The MIPS processor is a 32-bit little-endian processor.

  \nl

  This processor provides \textbf{32 integer registers}, from R0 to R31.

  \nl

  Nevertheless two registers have special meaning:

  \begin{itemize}[<+->]
    \item
      \textbf{R0}: Trash Register: this register returns zero on read and
      write operations are ignored.
    \item
      \textbf{R31}: Link Register: this register holds the return address
      of a subprogram.
  \end{itemize}

  Some other registers have special meanings from the compiler view, but
  this is just a basic MIPS rule:

  \begin{itemize}[<+->]
    \item
      \textbf{R2}: return value.
    \item
      \textbf{R29}: frame pointer.
    \item
      \textbf{R30}: global variables area pointer.
  \end{itemize}
\end{frame}

% 6)

\begin{frame}
  \frametitle{Special Registers}

  There are other special registers:

  \begin{itemize}[<+->]
    \item
      \textbf{HI and LO} are used for multiplications and divisions.
  \end{itemize}

  The MIPS processor has four registers very interesting for system
  programming:

  \begin{itemize}[<+->]
    \item
      \textbf{SR}: Status Register: used to distinguish the two context
      modes: user and supervisor.
    \item
      \textbf{CAUSE}: holds the cause of the interrupt/exception.
    \item
      \textbf{EPC}: Exception Program Counter: holds the program counter
      of the instruction that caused the exception.
    \item
      \textbf{BAR}: Bad Address Register: holds the address that caused
      the memory error.
  \end{itemize}
\end{frame}

% 7)

\begin{frame}
  \frametitle{RISC}

  While inventors of RISC said:

  \nl

  \textbf{``Reduced Instruction Set Computer''}

  \nl

  Other people said that was false and that the true definition was:

  \nl

  \textbf{``Reject Important Stuff into Compiler''}

  \nl

  These notices tend to explain the real goal of this processor because
  we will see that the MIPS processor respect this second statement.
\end{frame}

% 8)

\begin{frame}[containsverbatim]
  \frametitle{RISC vs CISC}

  Let's take an example to compare the assembly language of CISC and RISC
  processors.

  \nl

  We want to perform the operation below:

  \begin{verbatim}
    int         res;
    int         a = 21;
    int         b = 42;

    res = a + b;
  \end{verbatim}
\end{frame}

% 9)

\begin{frame}[containsverbatim]
  \frametitle{RISC vs CISC}

  On CISC processors:

  \begin{verbatim}
    add [res], [a], [b]
  \end{verbatim}

  On RISC processors:

  \begin{verbatim}
    Lw R4, [a]
    Lw R5, [b]
    Add R6, R4, R5
    Sw R6, [res]
  \end{verbatim}

  You can easily notice that CISC processors have only one instruction while
  RISC processors have four instructions to perform the same operation.
\end{frame}

% 10)

\begin{frame}
  \frametitle{Position of Assembly from High Level Languages}

  During the fifty's, CISC processors do not stop trying to provide
  more and more complex instructions to fit the high levels programming
  languages requirements.

  \nl

  RISC designers tried to do the exact opposite thing, to always stay
  near the hardware language.

  \nl

  Indeed, MIPS designers thought that while a CISC instruction ran in
  for example 4 cycles, the MIPS RISC microprocessor would be able to
  run the equivalent three instruction each taking 1 cycle.
\end{frame}

% 11)

\begin{frame}
  \frametitle{Philosophies}

  \begin{center}
    \pgfuseimage{philosophies}
  \end{center}
\end{frame}

%
% instruction formats
%

\section{Instruction Formats}

% 1)

\begin{frame}
  \frametitle{Overview}

  The MIPS processor classifies its 57 instructions into three groups.

  \begin{enumerate}[<+->]
    \item
      \textbf{R}: Register to register instructions: Register-Type.
    \item
      \textbf{I}: Memory and Branch instructions: Immediate-Type.
    \item
      \textbf{J}: Jump instructions: Jump-Type
  \end{enumerate}

  \nl

  Note that a branch instruction is a conditional jump while a jump
  instruction is an uncontional jump.
\end{frame}

% 2)

\begin{frame}
  \frametitle{Register Format}

  \begin{center}
    \pgfuseimage{r-format}
  \end{center}

  \begin{itemize}[<+->]
    \item
      \textbf{OPCODE}: the operation code.
    \item
      \textbf{RS}: the source register.
    \item
      \textbf{RT}: the alternative register: source register.
    \item
      \textbf{RD}: the destination register.
    \item
      \textbf{SHAM}: \textbf{SH}ift \textbf{AM}ount: used by
      shift instructions.
    \item
      \textbf{FUNC}: this function field is used to extend the number
      of available opcodes. Remember that opcode is an expensive resource.
  \end{itemize}
\end{frame}

% 3)

\begin{frame}
  \frametitle{Examples}

  Let's see some examples of register to register instructions.

  \begin{itemize}[<+->]
    \item
      Add Rd, Rs, Rt
    \item
      Addu Rd, Rs, Rt
    \item
      Sllv Rd, Rt, Rs
    \item
      Srl Rd, Rt, sham
  \end{itemize}
\end{frame}

% 4)

\begin{frame}
  \frametitle{Immediate Format}

  \begin{center}
    \pgfuseimage{i-format}
  \end{center}

  \begin{itemize}[<+->]
    \item
      \textbf{OPCODE}: the operation code.
    \item
      \textbf{RS}: the source register.
    \item
      \textbf{RT}: the alternative register: source/destination.
    \item
      \textbf{IMMED}: an immediate value.
  \end{itemize}
\end{frame}

% 5)

\begin{frame}
  \frametitle{Examples}

  Let's see some examples of immediate instructions.

  \begin{itemize}[<+->]
    \item
      Addi Rt, Rs, Immed
    \item
      Addiu Rt, Rs, Immed
    \item
      Andi Rt, Rs, Immed
    \item
      Ori Rt, Rs, Immed
  \end{itemize}
\end{frame}

% 6)

\begin{frame}
  \frametitle{Jump Format}

  \begin{center}
    \pgfuseimage{j-format}
  \end{center}

  \begin{itemize}[<+->]
    \item
      \textbf{OPCODE}: the operation code.
    \item
      \textbf{IMMED}: immediate value.
  \end{itemize}
\end{frame}

% 7)

\begin{frame}
  \frametitle{Examples}

  Let's see some examples of jump instructions.

  \begin{itemize}[<+->]
    \item
      J Immed
  \end{itemize}
\end{frame}

% 8)

\begin{frame}
  \frametitle{Opcodes}

  We saw that the opcode field took 6 bits.

  \nl

  We also saw that the register-type instructions had a function field
  to extend the number of opcodes. So one opcode field value is
  dedicated to the use of the extended function field.

  \nl

  This specific opcode value is zero: \textbf{000 000}. Whenever this
  opcode is used, the processor knows that the real opcode is located
  in the function field.

  \nl

  So the number of opcodes available on the MIPS is:

  \nl

  \textbf{opcodes}:

  $~~~~~= opcode~field - 1 + function~field$

  $~~~~~= 2^{6} - 1 + 2^{6}$

  $~~~~~= 127$
\end{frame}

%
% 9)
%

\begin{frame}[containsverbatim]
  \frametitle{Exercise}

  Let's write an optimised assembly code for the strlen function and
  calculate its number of cycles.

  \nl

  int \textbf{strlen}(const char* string);

  \begin{verbatim}
          Lw R5, 0(R29)                 ; R5 <- string

          Addi R3, R5, 1                ; R3 <- string + 1

    Loop: Lb R4, 0(R5)                  ; R4 <- *string
          Addiu R5, R5, 1               ; R5 <- string++
          Bne R4, R0, loop              ; while (*string != 0)

          Sub R2, R5, R3                ; R2 <- R5 - R3

          Jr R31                        ; return
  \end{verbatim}

  \textbf{Result}: $2 + N * 3 + 3 + 2$ cycles
\end{frame}

%
% instructions
%

\section{Instructions}

% 1)

\begin{frame}
  \frametitle{Arithmetic and Logic instructions}

  We will see how the processor interprets different instructions.

  \nl

  The processor also divides the instructions into two subcategories:
  the arithmetic instructions and the logic instructions.

  \nl

  Briefly, the arithmetic instructions handles overflow while logic
  one do not.

  \nl

  The MIPS processor is composed of \textbf{57 instructions}:

  \begin{itemize}[<+->]
    \item
      \textbf{33} arithmetic and logic instructions.
    \item
      \textbf{12} branch instructions.
    \item
      \textbf{7} memory access instructions.
    \item
      \textbf{5} system instructions.
  \end{itemize}

  \nl

  MIPS instructions are coded on 32-bit.
\end{frame}

% 2)

\begin{frame}
  \frametitle{Overflow Avoiding}

  Let's take the \textbf{Andi Rt, Rs, Immed} immediate-type
  instruction as example.

  \nl

  Note that overflow is ignored with logic instructions and with
  specific unsigned instructions (\textbf{u} suffixed) like:
  Addiu, Subu etc..

  \nl

  So in the case of the \textbf{Andi} instruction which is a logic
  instruction, the immediate will be interpreted as an unsigned value.

  \nl

  The 16-bit immediate value will be expanded to a 32-bit one just
  filling the 16 higher bits with zeros.

  \begin{center}
    \pgfuseimage{overflow-avoiding}
  \end{center}
\end{frame}

% 3)

\begin{frame}
  \frametitle{Overflow Handling}

  Let's take the \textbf{Addi Rt, Rs, Immed} immediate-type
  instruction as an example of an arithmetic one.

  \nl

  The non-presence of the \textbf{u} suffix introduces the overflow
  handling. This means that the immediate value will be considered
  as a signed value.

  \nl

  So, the signed value will be expanded as a 32-bit value taking care
  to report the sign bit.

  \begin{center}
    \pgfuseimage{overflow-handling}
  \end{center}
\end{frame}

% 4)

\begin{frame}[containsverbatim]
  \frametitle{Immediate Limitations}

  Let's see a common problem due to the immediate field limitations.

  \nl

  How to move the 32-bit value \textbf{0x87654321} into the R1 register.

  \begin{verbatim}
    Addi R1, R0, 0x8765
    Sll R1, R1, 16
    Addi R1, R1, 0x4321
  \end{verbatim}

  This operation being very useful, the designers of the MIPS processor
  decided to add a new instruction named \textbf{Lui} which does
  exactly the:

  \begin{itemize}
    \item
      Add Rn, R0, Immed
    \item
      Sll Rn, Rn, 16
  \end{itemize}
\end{frame}

% 5)

\begin{frame}[containsverbatim]
  \frametitle{Another Problem}

  Let's take a closer look to the sequence of these three instructions:

  \begin{verbatim}
    Addi R1, R0, 0x4567
    Sll R1, R1, 16
    Addi R1, R1, 0x89ab
  \end{verbatim}

  This sequence seems correct but it is not. Remember that the arithmetic
  operations handle the overflow.

  \nl

  The value \textbf{0x89ab} is equivalent to the binary value:
  \textbf{1000100110101011}.

  \nl

  You can notice that the higher bit is set so this number will be considered
  as a negative integer.
\end{frame}

% 6)

\begin{frame}[containsverbatim]
  \frametitle{Solution}

  The solution is simply to prefer a logic operation:

  \begin{verbatim}
    Addi R1, R0, 0x4567
    Sll R1, R1, 16
    Ori R1, R1, 0x89ab
  \end{verbatim}
\end{frame}

% 7)

\begin{frame}
  \frametitle{Memory Access}

  Memory accesses can be made for bytes, half-words and words.

  \nl

  The MIPS processor always aligns the data to the right of the registers.

  \begin{center}
    \pgfuseimage{memory-access}
  \end{center}
\end{frame}

% 8)

\begin{frame}[containsverbatim]
  \frametitle{Examples}

  These are other examples of memory instructions.

  \begin{verbatim}
    Lh Rt, Immed(Rs)
    Lhu Rt, Immed(Rs)
    Lw Rt, Immed(Rs)
    Sbu Rt, Immed(Rs)
    Sw Rt, Immed(Rs)
  \end{verbatim}
\end{frame}

% 9)

\begin{frame}
  \frametitle{Jump Instructions}

  Remember that we call branch instructions the conditional jumps
  and the jump instructions the unconditional ones.

  \nl

  Let's take the example of the \textbf{J} instruction. Remember that
  this instruction is of jump-type so is composed of 6 bits of opcode
  and 26 bits of immediate value.

  \begin{center}
    \pgfuseimage{jump-instruction}
  \end{center}

  This alignment system permit to access 256 Mb while each memory segment
  has a 256 Mb size.
\end{frame}

% 10)

\begin{frame}[containsverbatim]
  \frametitle{Examples}

  Let's see other jump instructions:

  \begin{verbatim}
    Jr Rs
    Jal Immed
    Jalr Rs
  \end{verbatim}

  To return from a subprogram we just have to do:

  \begin{verbatim}
    Jr R31
  \end{verbatim}
\end{frame}

% 11)

\begin{frame}
  \frametitle{Branch Instructions}

  Let's take as example the instruction: \textbf{Beq Rs, Rt, Label}.

  \begin{enumerate}
    \item
      \textbf{Rs == Rt}: the next instruction address will be the label.
    \item
      \textbf{Rs != Rt}: the next instruction address is simply the following
      so the current instruction address plus four bytes.
  \end{enumerate}

  \nl

  In the case of a successful branch, the new instruction address is
  calculated like this:

  \nl

  $~~~~~target~address = branch~instruction~address + 4 + (Immediate << 2)$

  \nl

  You can notice that the immediate will be interpreted as a signed value.

  \nl

  This is useful to permit forward an backward jumps.
\end{frame}

% 12)

\begin{frame}[containsverbatim]
  \frametitle{Examples}

  Let's see some examples of branch instructions.

  \begin{verbatim}
    Bne Rs, Rt, Immed
    Bltz Rs, Immed
    Blez Rs, Immed
    Bgtz Rs, Immed
    Bgez Rs, Immed
  \end{verbatim}
\end{frame}

% 13)

\begin{frame}
  \frametitle{Designers Choices}

  We saw the different type and instructions of the MIPS processor. But
  why the designers decided to provide this instruction but not this one?

  \nl

  First of all, the instructions set is not build without reflexion
  but on statistics, rules and logic:

  \begin{itemize}[<+->]
    \item
      \textbf{MIX}
    \item
      \textbf{Amdhal Rule}
  \end{itemize}
\end{frame}

% 14)

\begin{frame}
  \frametitle{MIX}

  The MIXs are kind of statistics made by researchers and companies.

  \nl

  The MIXs classify, sort the instructions by their use frequency.

  \nl

  For example the instruction \textbf{Strlen Rd, Rs} which computes
  the length of the string located at \textbf{Rs} and store the result
  into \textbf{Rd} may be used for example 1 percent of the execution
  time of a process.

  \nl

  In this case, if the designers decide to not implement this instruction
  what will be the performance loss?
\end{frame}

% 15)

\begin{frame}
  \frametitle{Performances}

  Consider that this instruction running in one cycle can be replaced
  by for example two other instructions each running in one cycle.

  \nl

  What will be the real performance loss?

  \nl

  \textbf{loss}:

  $~~~~~= cycle~loss * frequency$

  $~~~~~= 50\% * 1\%$

  $~~~~~= 0.005$

  $~~~~~= 0.5\%$

  \nl

  The MIXs can help to detect instructions with very low use frequencies, so
  the designers can decide not to include these instructions in the
  processor's instructions set.
\end{frame}

% 16)

\begin{frame}
  \frametitle{Amdhal Rule}

  The Amdhal Rule says:

  \nl

  \textbf{``The real benefit depends on the cost.''}

  \nl

  Let's take a well-known analogy:

  \begin{center}
    \pgfuseimage{amdhal-rule}
  \end{center}

  To conclude:

  \nl

  $~~~~~real~earning = earning * use$
\end{frame}

% 17)

\begin{frame}
  \frametitle{RISC Performance Rules}

  The two caracteristics which make a processor performant are:

  \begin{itemize}
    \item
      Number of cycles per instruction.
    \item
      Clock frequency.
  \end{itemize}

  So the processor performance can be computed as below:

  \nl

  \textbf{performance}:

  $~~~~~= frequency / number~of~cyles~per~instruction$

  $~~~~~= F / CPI$

  \nl

  We will try to build a CPI equals to 1 leading to a coherent and peformant
  RISC processor.
\end{frame}

% 18)

\begin{frame}
  \frametitle{Questions}

  Any question?

  \begin{itemize}[<+->]
    \item
      Why the \textbf{Subi} instruction does not exist?
    \item
      Why the \textbf{Jr} instruction is a register-type instruction
      while its goal is to perform a jump?
    \item
      Why the \textbf{Nori} instruction does not exist?
    \item
      Why the \textbf{Sll} instruction is a register-type instruction
      while it uses an immediate?
  \end{itemize}

  The answers are:

  \begin{itemize}[<+->]
    \item
      Opcode is a very expensive resource.
    \item
      Most instructions are useless because the designed operation can be
      made with another instruction.
    \item
      \textbf{MIXs} and the \textbf{Amdhal Rule} are used to decide which
      instruction to insert into the instructions set.
  \end{itemize}
\end{frame}

%
% 19)
%

\begin{frame}[containsverbatim]
  \frametitle{Exercise}

  Let's try to write the assembly source code for the vector function
  which computes an addition between the elements of two arrays and stores
  the result in another one.

  \nl

  The assembly code must be optimised and runnable on MIPS microprocessors.

  \nl

  void \textbf{vector}(int* a, int* b, int *c, unsigned int size);

  \begin{verbatim}
    for (i = 0; i < size; i++)
      c[i] = 2 * a[i] + 3 * b[i];
  \end{verbatim}
\end{frame}

%
% 20)
%

\begin{frame}[containsverbatim]
  \frametitle{Correction}

  \begin{verbatim}
          Lw R3, 0(R29)                 ; R3 <- a
          Lw R4, 4(R29)                 ; R4 <- b
          Lw R5, 8(R29)                 ; R5 <- c
          Lw R6, 12(R29)                ; R6 <- size
          Sll R6, R6, 2                 ; R6 <- size * 4
          Add R7, R5, R6                ; R7 <- R5 + size * 4
    Loop: Lw R13, 0(R3)                 ; R13 <- *a
          Lw R14, 0(R4)                 ; R14 <- *b
          Add R15, R13, R14             ; R15 <- *a + *b
          Sll R15, R15, 1               ; R15 <- (*a + *b) * 2
          Add R15, R15, R14             ; R15 <- (*a + *b) * 2 + *b
          Sw R15, 0(R5)                 ; *c <- R15
          Addi R3, R3, 4                ; a <- a + 1
          Addi R4, R4, 4                ; b <- b + 1
          Addi R5, R5, 4                ; c <- c + 1
          Bne R5, R7, loop              ; while (R5 != R7)
          Jr R31                        ; return
  \end{verbatim}

  \textbf{Result}: $6 + N * 10 + 10 + 1$ cycles
\end{frame}

%
% addressing
%

\section{Addressing}

% 1)

\begin{frame}
  \frametitle{Overview}

  The MIPS processor studied does not have any virtual memory in its basic
  realisation.

  \nl

  More complex versions do have but we will only study basic ones to
  avoid complex pipelines.

  \nl

  Addresses are represented by 32-bit integers.

  \nl

  The memory accesses must be aligned, meaning that the object's address
  (byte, half-word, word) must always be a multiple of the object size.

  \nl

  For example an half-word must always be accessed on an address multiple
  of two.
\end{frame}

% 2)

\begin{frame}
  \frametitle{Endianness}

  The MIPS RISC processor uses the little-endian memory organisation.

  \begin{center}
    \pgfuseimage{endianness}
  \end{center}
\end{frame}

% 3)

\begin{frame}
  \frametitle{Memory Layout}

  The memory is segmented to divide the address space into two segments.

  \begin{center}
    \pgfuseimage{memory-layout}
  \end{center}
\end{frame}

%
% pipeline
%

\section{Pipeline}

% 1)

\begin{frame}
  \frametitle{Execution Context}

  We will study in the section the different instructions from another
  point of view to extract a generic execution context.

  \nl

  Then, having an execution context, we will be able to study the pipeline
  and each instruction in it to find and solve some problems.
\end{frame}

% 2)

\begin{frame}
  \frametitle{Add Rd, Rs, Rt}

  \begin{enumerate}[<+->]
    \item
      Reads the instruction.
    \item
      Decodes the operands.
    \item
      Extracts the operands: \textbf{Rs}, \textbf{Rt}
    \item
      Performs the operation: \textbf{+}.
    \item

    \item
      Writes the result: \textbf{Rd}
    \item
      Computes the next instruction address.
  \end{enumerate}
\end{frame}

% 3)

\begin{frame}
  \frametitle{Lw Rt, Immed(Rs)}

  \begin{enumerate}[<+->]
    \item
      Reads the instruction.
    \item
      Decodes the operands.
    \item
      Extracts the operands: \textbf{Immed}, \textbf{Rs}
    \item
      Performs the operation: \textbf{+}.
    \item
      Memory access: read.
    \item
      Writes the result: \textbf{Rt}
    \item
      Computes the next instruction address.
  \end{enumerate}
\end{frame}

% 4)

\begin{frame}
  \frametitle{Sw Rt, Immed(Rs)}

  \begin{enumerate}[<+->]
    \item
      Reads the instruction.
    \item
      Decodes the operands.
    \item
      Extracts the operands: \textbf{Immed}, \textbf{Rs}, \textbf{Rt}
    \item
      Performs the operation: \textbf{+}.
    \item
      Memory access: write.
    \item

    \item
      Computes the next instruction address.
  \end{enumerate}
\end{frame}

% 5)

\begin{frame}
  \frametitle{Jr Rs}

  \begin{enumerate}[<+->]
    \item
      Reads the instruction.
    \item
      Decodes the operands.
    \item
      Extracts the operands: \textbf{Rs}
    \item

    \item

    \item

    \item
      Computes the next instruction address.
  \end{enumerate}
\end{frame}

% 6)

\begin{frame}
  \frametitle{Goal}

  The goal of a processor designer is to choose:

  \begin{itemize}[<+->]
    \item
      the instructions set.
    \item
      the execution context.
  \end{itemize}

  \nl

  We saw that any MIPS instruction could be described with a simple
  seven stages execution context.
\end{frame}

% 7)

\begin{frame}
  \frametitle{Pipeline Overview}

  Pipeline technique was directly imported from hydrocarbure industry,
  using tops to separate stages.

  \begin{center}
    \pgfuseimage{pipeline-overview}
  \end{center}

  \nl

  We successfully built a pipeline with a CPI equals to 1 but
  limiting the frequency. This is not a good solution because the
  performances will not be good enough.
\end{frame}

% 8)

\begin{frame}
  \frametitle{Example}

  Let's take an example of pipeline which execute one instruction per 100 ns:
  \textbf{1 instruction / 100 ns = 10 Mhz}.

  \nl

  If we introduce in this pipeline a top to cut the pipeline in two stages,
  the instruction will be executed for example in 105 ns due to the overhead:
  \textbf{2 instruction / 105 ns = 20 Mhz}.

  \nl

  This is the beauty of the pipeline processors. More stages, more frequency
  but with more problems we will see later.

  \begin{itemize}
    \item
      \textbf{Latency}: 105 ns = 2 cycles
    \item
      \textbf{Frequency}: 1 instruction / 50 ns = 20 Mhz
    \item
      \textbf{CPI}: 1
  \end{itemize}

\end{frame}

% 9)

\begin{frame}
  \frametitle{Explanations}

  Rather than making one cyle for the entire pipeline, we will
  calibrate the clock on stages to be able to get out an instruction
  of the pipeline every cycle.

  \nl

  The tops used by the processor pipelines are registers.

  \nl

  At each clock, each register pushs its contents into the pipeline,
  to the right.

  \nl

  The stages of the pipeline \alert{must} be balanced to not slow
  down the entire pipeline.

  \nl

  Moreover a bad balancing may lead to loss of data because when the clock
  will occur the data will not be stored into the destination register yet.
\end{frame}

% 10)

\begin{frame}
  \frametitle{Balancing}

  Let's take a look to a bad balancing and it consequencies.

  \begin{center}
    \pgfuseimage{pipeline-balancing}
  \end{center}

  Be careful, tops cutting has nothing to do with execution context.
\end{frame}

% 11)

\begin{frame}
  \frametitle{Rules}

  The pipelining rules are:

  \begin{itemize}[<+->]
    \item
      The pipeline stages must be balanced.
    \item
      The frequency must always be calibrated on the longest stage.
  \end{itemize}
\end{frame}

% 12)

\begin{frame}
  \frametitle{Pipeline}

  The designers of the MIPS processor decided to cut the pipeline as
  shown below.

  \nl

  This is the real MIPS pipeline from 1980.

  \begin{enumerate}
    \item
      \textbf{IFC}: Instruction Fetch: the instruction is read from the
      memory
    \item
      \textbf{DEC}: Instruction Decode: the operands are decoded and
      extracted while the next instruction address is computed.
    \item
      \textbf{EXE}: Execute: an operation is performed.
    \item
      \textbf{MEM}: Memory: a data memory access is made.
    \item
      \textbf{WBK}: Write Back: the result is stored into the destination
      register.
  \end{enumerate}
\end{frame}

% 13)

\begin{frame}
  \frametitle{Moore Law}

  The remaining question is: what is the largest stage in this pipeline.

  \nl

  The Moore Law shows that processors frequency will grow exponentially,
  but what is the memory evolution?

  \begin{center}
    \pgfuseimage{moore-law}
  \end{center}
\end{frame}

% 14)

\begin{frame}
  \frametitle{Largest Stage}

  So the largest pipeline stages will be IFC and MEM which do memory
  accesses.

  \nl

  Note that such a pipeline will not be possible nowadays because of the
  Moore law. The difference between the processor speed and the memory speed
  is constantly increasing.

  \nl

  For example, a good idea for a modern processor would be to cut the
  MEM stage into three sub-stages.
\end{frame}

% 15)

\begin{frame}
  \frametitle{Pipeline Views}

  We can watch a pipeline from two different views:

  \begin{center}
    \pgfuseimage{pipeline-views}
  \end{center}

  \begin{itemize}
    \item
      The first describes an instruction state.
    \item
      The second describes the pipeline state.
  \end{itemize}
\end{frame}

% 16)

\begin{frame}
  \frametitle{Hardware}

  We saw that the pipeline technique leads to very good performances because
  an instruction gets out every cycle.

  \nl

  Nevertheless this performance does not come without problems.

  \nl

  First of all, the hardware must be duplicated because a hardware must
  be specific to a stage to avoid overwrittings.

  \nl

  In other words stages are composed of inputs and outputs registers.
\end{frame}

% 17)

\begin{frame}
  \frametitle{Memory}

  This is for the same reason that there are two distinct memory buses
  to access memory instructions and memory data because the IFC and MEM
  stages have to access memory at the same time:

  \begin{itemize}[<+->]
    \item
      \textbf{Instruction-Access}: location of the memory instruction.
    \item
      \textbf{Instruction}: instruction.
    \item
      \textbf{Data-Access}: location of the memory object.
    \item
      \textbf{Data}: object.
  \end{itemize}
\end{frame}

% 18)

\begin{frame}
  \frametitle{Pipeline Rules}

  These are the rules to follow to build a correct pipeline:

  \begin{enumerate}[<+->]
    \item
      Stages must be balanced.
    \item
      Stages must be separated by registers.
    \item
      Hardware must be specificly used by a stage and never by another.
  \end{enumerate}

  \nl

  Notice that registers largely compose a processor.
\end{frame}

% 19)

\begin{frame}
  \frametitle{Detailed Pipeline}

  \begin{center}
    \pgfuseimage{detailed-pipeline}
  \end{center}
\end{frame}

% 20)

\begin{frame}
  \frametitle{Exercises}

  Make the detailed pipeline for these instructions:

  \begin{itemize}[<+->]
    \item
      \textbf{Sllv Rd, Rs, Rt}: nothing special
    \item
      \textbf{Lw Rd, Immed(Rs)}: \alert{multiplexer}:
      RES or memory data into DATA.
  \end{itemize}
\end{frame}

% 21)

\begin{frame}
  \frametitle{Next Instruction Address}

  Let's take a closer look to the next instruction address computation.

  \nl

  The next instruction is computed in the DEC stage. So between
  the time the instruction gets in the pipeline and its next instruction
  address computation, one instruction was added to the pipeline.

  \begin{center}
    \pgfuseimage{next-instruction-stream}
  \end{center}

  \nl

  When performing a branch the pipeline will contain an unwanted
  instruction.
\end{frame}

% 22)

\begin{frame}
  \frametitle{Delay Slot}

  This unwanted instruction is called \textbf{delay slot}.

  \nl

  There are two solutions to solve the delay slot problem:

  \begin{itemize}[<+->]
    \item
      Just do not touch to this instruction which will be executed
      in the pipeline.
    \item
      Cancel, remove this instruction from the pipeline.
  \end{itemize}
\end{frame}

% 23)

\begin{frame}
  \frametitle{Solution}

  The solution is to reject important stuff into the compiler.

  \nl

  Researchs made showed that the compiler is able to fill one delay
  slot in 75 percent of cases and to fill two delay slots in only
  5 percent of cases. After that there is no chance to fill the delay
  slots.

  \nl

  After that, if a compiler is not able to fill the delay slots
  with useful instructions it will inserts a NOP instruction to avoid
  problems.

  \nl

  Question: \textit{What are the consequences if we put the next
    instruction address computation into the EXE stage?}
\end{frame}

% 24)

\begin{frame}
  \frametitle{Next Instruction Address Computation}

  Let's review the next instruction address formula:

  \nl

  There are two cases:

  \begin{itemize}[<+->]
    \item
      \textbf{Rs == Rt}:
        $target~address = branch~instruction~address + 4 + (Immediate * 4)$
    \item
      \textbf{Rs != Rt}:
        $target~address = next~instruction~address$
  \end{itemize}

  \nl

  First of all, the part of the formula: \textbf{branch address + 4}
  was already computed between the IFC stage and the DEC stage because
  the previous instruction computed the next instruction address.
\end{frame}

% 25)

\begin{frame}
  \frametitle{DEC stage}

  We saw that the DEC stage had to decode and extract operands while
  computing the next instruction address.

  \begin{center}
    \pgfuseimage{decode-branch}
  \end{center}
\end{frame}

% 26)

\begin{frame}
  \frametitle{Very Bad Balancing}

  We can notice that the stages balancing is absolutly not respected.

  \nl

  Indeed, while the EXE stage performs an addition, the DEC stage performs:

  \begin{itemize}[<+->]
    \item
      \textbf{Multiplication}: Immed * 4
    \item
      \textbf{Comparison}
    \item
      \textbf{Multiplexer}
    \item
      \textbf{Addition}: Multiplixer result + instruction address
  \end{itemize}

  \nl

  So we have to find solutions to balance this stage with others.
\end{frame}

% 27)

\begin{frame}
  \frametitle{Multiplication Solution}

  First of all, we can handle the multiplication \textbf{Immed * 4}
  in a more quickly way.

  \nl

  In fact, this muliplication operation will takes exactly zero time
  to compute.

  \nl

  This solution is to add an hardware just to perform a multiplexing
  between a multiplication by four and the number four.

  \begin{center}
    \pgfuseimage{decode-multiplexer}
  \end{center}
\end{frame}

% 28)

\begin{frame}
  \frametitle{Still Problem}

  We success to optimise the multiplication so let's now compare the
  two stages DEC and EXE.

  \begin{center}
    \pgfuseimage{comparison-decode-execute-1}
  \end{center}
\end{frame}

% 29)

\begin{frame}
  \frametitle{Parallel Computations}

  We could parallelise the computations for the addition and the
  other stuff and then perform the multiplexing.

  \begin{center}
    \pgfuseimage{comparison-decode-execute-2}
  \end{center}
\end{frame}

% 30)

\begin{frame}
  \frametitle{Instruction Formats' Beauty}

  You must had noticed that the different instruction formats all placed
  the source register and the alternative register at the same offsets.

  \nl

  The result effect is that we are not longer forced to wait the decode
  operations because the can directly extract the operands to perform
  the comparison.

  \begin{center}
    \pgfuseimage{comparison-decode-execute-3}
  \end{center}

  The result is almost perfect.
\end{frame}

% 31)

\begin{frame}
  \frametitle{Strutural Problem Resolved}

  Let's see the DEC stage in its final state.

  \begin{center}
    \pgfuseimage{decode-stage}
  \end{center}
\end{frame}

% 32)

\begin{frame}[containsverbatim]
  \frametitle{But...}

  Is there any other problem?

  \nl

  Think about this sequence of instructions:

  \begin{verbatim}
    Beq R1, R2, somewhere
    Beq R3, R4, anywhere
  \end{verbatim}

  \nl

  MIPS architecture specification tells that two following branchs are
  not allowed.
\end{frame}

% 33)

\begin{frame}[containsverbatim]
  \frametitle{Exercise}

  \textbf{Bltzal Rs, Label}: Branch if Less Than Zero And Link

  \nl

  This instruction computes the next instruction address and stores
  the \textbf{current address + 4} into the R31 register.

  \nl

  Note that the caller must save the R31 register on the stack before
  making any call to subprograms.

  \begin{verbatim}
    Sw R31, X(R29)
    Branch
    Lw R31, X(R29)
  \end{verbatim}

  \nl

  Let's make the detailed pipeline of this instruction.
\end{frame}

%
% dependencies
%

\section{Dependencies}

% 1)

\begin{frame}
  \frametitle{Overview}

  We will see in this section that there are other inherent pipeline
  problems.

  \nl

  We will try to solve them while respecting the RISC spirit but
  this will not be always possible.
\end{frame}

% 2)

\begin{frame}[containsverbatim]
  \frametitle{Example}

  Let's get a simple example and study it in the pipeline:

  \begin{verbatim}
    Add R3, R1, R2
    Add R5, R3, R4
  \end{verbatim}

  \begin{center}
    \pgfuseimage{dependency-problem-overview}
  \end{center}

  You can notice that the pipeline cannot work with instruction dependencies.
\end{frame}

% 3)

\begin{frame}
  \frametitle{Solutions}

  So, there are two solutions:

  \begin{itemize}[<+->]
    \item
      To respect the RISC spirit and reject the important stuff into the
      compiler.
    \item
      Or to handle these situations directly into the processor.
  \end{itemize}

  \nl

  Needless to say, the compiler will handle it.

  \nl

  The compiler will have to handle three slots between two instructions
  because every instruction needs the operands in the DEC stage and the
  destination registers are always generated into the WBK stage.

  \nl

  We saw earlier that the compiler was not correctly able to fill more than
  one slot meaning that in 95 percent of cases two slots will be filled
  with no operation instructions.

  \nl

  This solution is absolutely unacceptable.

  \nl

  So the processor has to handle dependencies by itself.
\end{frame}

% 4)

\begin{frame}
  \frametitle{The Solution}

  We will try to handle dependencies into the pipeline.

  \nl

  First, in the case of computation instruction like addition, substraction
  etc.., it is absolutely useless to extract operands into the DEC stage.

  \nl

  Indeed, the operands are only used in the EXE stage.

  \nl

  Moreover, in the same context, the destination register is ready
  at the end of the EXE stage, so needless to wait the end of the
  WBK stage to get it.

  \begin{center}
    \pgfuseimage{dependency-solution-overview}
  \end{center}

  This system works but needs additional hardware.
\end{frame}

% 5)

\begin{frame}
  \frametitle{Detailed Pipeline}

  \begin{center}
    \pgfuseimage{dependency-detailed-pipeline}
  \end{center}
\end{frame}

% 6)

\begin{frame}
  \frametitle{Bypasses Overview}

  The additional hardware is composed of two components:

  \begin{enumerate}[<+->]
    \item
      Multiplexers.
    \item
      Dependencies Detectors.
  \end{enumerate}

  \nl

  These components compose what we call \textbf{bypasses}.

  \nl

  The dependencies detectors will check if consecutive instructions
  are dependent.

  \nl

  The result will activate or not the multiplexer which will select
  the correct input register: \textbf{RES\_RE} or \textbf{SOPER\_RD}
  in our example.
\end{frame}

% 7)

\begin{frame}[containsverbatim]
  \frametitle{Dependencies}

  Let's enumerate some instruction sequences to isolate dependency
  cases.

  \begin{verbatim}
    Add R3, R2, R1
    Jr R3
  \end{verbatim}

  \begin{verbatim}
    Add R3, R2, R1
    Add R5, R4, R3
  \end{verbatim}

  \begin{verbatim}
    Lw R3, 0(R5)
    Add R5, R4, R3
  \end{verbatim}

  \begin{verbatim}
    Lw R3, 0(R5)
    Beq R3, R0, label
  \end{verbatim}
\end{frame}

% 8)

\begin{frame}
  \frametitle{Bypasses}

  The MIPS with a five stage pipeline only needs four bypasses to
  handle every possible dependency.

  \nl

  There are dependencies of order: \textbf{1}, \textbf{2} and \textbf{3}.

  \begin{center}
    \pgfuseimage{bypasses}
  \end{center}
\end{frame}

% 9)

\begin{frame}
  \frametitle{Worst Case}

  Of course the worst case happens when a branch instruction depends on
  a memory instruction.

  \nl

  This case automatically leads to two stalls.
\end{frame}

% 10)

\begin{frame}[containsverbatim]
  \frametitle{Example - Source Code}

  Let's see an example.

  \begin{verbatim}
    int         array[size];
    int         i;

    for (i = 0; i != size; i++)
      array[i] = array[i] * 2;
  \end{verbatim}

  In assembly:

  \begin{verbatim}
    loop: Lw R3, 0(R5)
          Sll R3, R3, 1
          Sw R3, 0(R5)
          Addiu R5, R5, 4
          Addiu R8, R8, 1
          Bne R8, R9, loop
          Nop
  \end{verbatim}
\end{frame}

% 11)

\begin{frame}
  \frametitle{Example - Pipeline}

  Let's see the pipeline for this instructions sequence.

  \begin{center}
    \pgfuseimage{example-pipeline}
  \end{center}
\end{frame}

% 12)

\begin{frame}[containsverbatim]
  \frametitle{Example - Rescheduling}

  The performances for our examples were:

  \nl

  $CPI = 9 / 7$

  $CPIu = 9 / 6$

  \nl

  Let's try to reorganise the instructions sequence:

  \begin{verbatim}
    loop: Lw R3, 0(R5)
          Addiu R8, R8, 1
          Sll R3, R3, 1
          Sw R3, 0(R5)
          Bne R8, R9, loop
          Addiu R5, R5, 4
  \end{verbatim}

  \nl

  $CPI = 6 / 6$

  $CPI = 6 / 6$ \alert{= 1}
\end{frame}

% 13)

\begin{frame}[containsverbatim]
  \frametitle{Trash Register Dependencies}

  Let's take another example:

  \begin{verbatim}
    Lw R0, 0(R5)
    Beq R1, R0, label
  \end{verbatim}

  This dependency should produces two stalls.

  \nl

  But it is obvious that this dependency, in fact, does not exist.

  \nl

  The trash register must be handled as a never-dependent register.
\end{frame}

% 14)

\begin{frame}
  \frametitle{Exercise - Strlen Part One}

  \begin{center}
    \pgfuseimage{strlen-part-1}
  \end{center}
\end{frame}

% 15)

\begin{frame}
  \frametitle{Exercise - Strlen Part Two}

  Let's produce a better assembly code.

  \begin{center}
    \pgfuseimage{strlen-part-2}
  \end{center}
\end{frame}

% 16)

\begin{frame}
  \frametitle{Exercise - Strlen Part Three}

  Another optimised code.

  \begin{center}
    \pgfuseimage{strlen-part-3}
  \end{center}
\end{frame}

% 17)

\begin{frame}
  \frametitle{Exercise - Strlen Part Four}

  Let's now reschedule instructions.

  \begin{center}
    \pgfuseimage{strlen-part-4}
  \end{center}
\end{frame}

% 18)

\begin{frame}
  \frametitle{Conclusion}

  You could notice that the CPI increases while we try to
  optimize the assembly code.

  \nl

  Indeed, the CPI is not interesting in our example.

  \nl

  We simply tried to reduce the number of instructions in the loop
  to reduce the cycles so the CPI but this was not possible.

  \nl

  Indeed, in the last examples, the loop will take exactly five cycles
  and we showed this was not optimisable.
\end{frame}

% 19)

\begin{frame}
  \frametitle{Questions}

  \textit{Why not putting a bypass into the MEM stage?}

  \nl

  The MEM stage is the largest of the pipeline, the processor cycle
  is based on this stage.

  \nl

  If we put a bypass into it, we introduce a multiplexer, so the MEM
  stage will take much time and the processor frequency will be slow down.

  \nl

  Let's see the critical stages:

  \begin{enumerate}
    \item
      \textbf{IFC} and \textbf{MEM}
    \item
      \textbf{DEC}
    \item
      \textbf{EXE}
    \item
      \textbf{WBK}
  \end{enumerate}
\end{frame}

%
% optimisations
%

\section{Optimisations}

% 1)

\begin{frame}
  \frametitle{Overview}

  We will see in this section different optimisations which can be
  used either by the assembly programmer or by the compiler.

  \nl

  We already saw an optimisation consisting in the reorganisation
  of instructions to avoid instruction dependencies.
\end{frame}

% 2)

\begin{frame}[containsverbatim]
  \frametitle{Assembly Code}

  We will take an example to show optimisations through this section.

  \begin{verbatim}
    for (i = 0; i != size; i++)
      v[i] = 2 * v[i];
  \end{verbatim}

  \begin{itemize}
    \item
      \textbf{R5}: the vector \textbf{v} address.
    \item
      \textbf{R6}: the vector size \textbf{size}.
    \item
      \textbf{R7}: the increment \textbf{i}.
  \end{itemize}

  \begin{verbatim}
    loop: Lw R8, 0(R5)
          Sll R8, R8, 1
          Sw R8, 0(R5)
          Addiu R5, R5, 4
          Addi R7, R7, 1
          Bne R7, R6, loop
          Nop
  \end{verbatim}
\end{frame}

% 3)

\begin{frame}
  \frametitle{Performances}

  There are currently two stalls:

  \begin{itemize}[<+->]
    \item
      Between \textbf{Lw} and \textbf{Sll}.
    \item
      Between \textbf{Addi} and \textbf{Bne}.
  \end{itemize}

  and the performances are:

  \nl

  $CPI = 9 / 7$

  $CPIu = 9 / 6$
\end{frame}

% 4)

\begin{frame}[containsverbatim]
  \frametitle{Better Assembly Code}

  First, we will try to optimise the assembly source code because
  some instructions are needless.

  \begin{itemize}
    \item
      \textbf{R7}: the vector end address.
  \end{itemize}

  \begin{verbatim}
    loop: Lw R8, 0(R5)
          Sll R8, R8, 1
          Sw R8, 0(R5)
          Addiu R5, R5, 4
          Bne R5, R7, loop
          Nop
  \end{verbatim}

  \nl

  $CPI = 8 / 6$

  $CPIu = 8 / 5$
\end{frame}

% 5)

\begin{frame}[containsverbatim]
  \frametitle{Rescheduling}

  The \textbf{Addiu} instruction is independent so we can use it to
  avoid other instruction dependencies while keeping this instruction
  far away from the \textbf{Bne} instruction which depends on it.

  \begin{verbatim}
    loop: Lw R8, 0(R5)
          Addiu R5, R5, 4
          Sll R8, R8, 1
          Sw R8, -4(R5)
          Bne R5, R7, loop
          Nop
  \end{verbatim}

  \nl

  $CPI = 6 / 6$

  $CPIu = 6 / 5$
\end{frame}

% 6)

\begin{frame}[containsverbatim]
  \frametitle{Eliminating The Delay Slot}

  Let's try to fill the delay slot with a useful instruction to
  increase the CPI.

  \begin{verbatim}
    loop: Lw R8, 0(R5)
          Addiu R5, R5, 4
          Sll R8, R8, 1
          Bne R5, R7, loop
          Sw R8, -4(R5)
  \end{verbatim}

  \nl

  $CPI = 5 / 5$

  $CPIu = 5 / 5$ \alert{= 1}
\end{frame}

% 7)

\begin{frame}
  \frametitle{Software Pipeline Theory}

  Compilers use another optimisation named \textbf{Software Pipeline}.

  \nl

  This technique is very useful with little sequence of instructions into
  a loop because each instruction depends on the previous one.

  \nl

  We could analyse loop's bodies instructions like this:

  \begin{enumerate}[<+->]
    \item
      \textbf{L}oad: first, an element of an array for example is read from
      the memory.
    \item
      \textbf{O}peration: then, an operation is performed on this element.
    \item
      \textbf{S}tore: finally, the computed value is stored in memory.
  \end{enumerate}
\end{frame}

% 8)

\begin{frame}
  \frametitle{Software Pipeline Views}

  We can view the loop body execution from two different points of view:

  \begin{center}
    \pgfuseimage{software-pipeline}
  \end{center}
\end{frame}

% 9)

\begin{frame}[containsverbatim]
  \frametitle{Software Pipelining}

  So, rather than handling on element per iteration, we will handle
  three elements per iteration to increase the number of instructions
  between dependences.

  \nl

  But, finally, an element will be treated during three consecutive
  iterations, to perform the three steps: Load, Operation and Store.

  \begin{verbatim}
    loop: Sw R10, -8(R5)
          Sll R10, R9, 1
          Lw R9, 0(R5)
          Addiu R5, R5, 4
          Bne R5, R7, loop
          Nop
  \end{verbatim}

  Of course, the first and last elements will be handled outside the loop.

  \nl

  $CPI = 8 / 6$

  $CPIu = 8 / 5$
\end{frame}

% 10)

\begin{frame}[containsverbatim]
  \frametitle{Optimisation}

  Of course, stalls remain but we can easily optimise this source code
  using instructions rescheduling:

  \begin{verbatim}
    loop: Sw R10, -8(R5)
          Sll R10, R9, 1
          Addiu R5, R5, 4
          Bne R5, R7, loop
          Lw R9, -4(R5)
  \end{verbatim}

  \nl

  $CPI = 5 / 5$

  $CPIu = 5 / 5$ \alert{= 1}

  \nl

  Note that the \textbf{Sw}, \textbf{Sll} and \textbf{Lw} instructions
  are no longer dependent because they handle different array elements.
\end{frame}

% 11)

\begin{frame}
  \frametitle{Loop Unroll Theory}

  \textbf{Analogy}: When you take your car to go to school you move
  about 1000 kg metal for a 80 kg body. So the ratio is 80/1000. But
  if you take the bus, the ratio is about: 2000/5000.

  \nl

  For our example, the loop body is composed of five instructions, but
  two of them are used to manage the loop:

  \begin{itemize}[<+->]
    \item
      Addiu R5, R5, 4
    \item
      Bne R5, R7, loop
  \end{itemize}

  The idea behing the loop unrolling is to handle for exemple two
  iterations per loop to pay the overhead only once.
\end{frame}

% 12)

\begin{frame}[containsverbatim]
  \frametitle{Loop Unrolling}

  \begin{verbatim}
    loop: Lw R8, O(R5)
          Lw R10, 4(R5)
          Sll R8, R8, 1
          Sll R10, R10, 1
          Sw R8, 0(R5)
          Sw R10, 4(R5)
          Addiu R5, R5, 8
          Bne R5, R7, loop
          Nop
  \end{verbatim}

  When optimised we have:

  \nl

  \textbf{Performances}: $8 cycles / iteration$

  \textbf{Performances}: $4 cycles / element$
\end{frame}

% 13

\begin{frame}
  \frametitle{Conclusion}

  This technique can be widely used to increase the unrolling ratio.

  \nl

  In fact this technique tends to remove the cost of the loop management
  instructions.

  \nl

  Moreover, this technique increase the number of instructions into the
  loop body so this makes more possibilities to avoid dependencies
  between instructions.
\end{frame}

% 14)

\begin{frame}[containsverbatim]
  \frametitle{Exercises}

  Let's take the following source code which compute the absolute value
  on every element of an array:

  \begin{verbatim}
    for (i = 0; i != size; i++)
      v[i] = abs(v[i]);
  \end{verbatim}

  The equivalent non-optimised assembly code is:

  \begin{verbatim}
     loop: Lw R8, 0(R5)
           Bgez R8, endif
           Nop
           Sub R8, R0, R8
           Sw R8, 0(R5)
    endif: Addiu R5, R5, 4
           Bne R5, R7, loop
           Nop
  \end{verbatim}

  Let's optimise this assembly code: source code, instructions rescheduling,
  software pipelining and finally loop unrolling.
\end{frame}

%
% advanced pipelines
%

\section{Advanced Pipelines}

% 1)

\begin{frame}
  \frametitle{Overview}

  In this section we will study advanced pipelines including superpipelines,
  pipelines handling complex instructions like multiplication, division
  and finally superscalar microprocessors.
\end{frame}

% 2)

\begin{frame}
  \frametitle{Superpipelines}

  We called superpipelines or deep pipelines, extended pipelines which
  raise very huge sizes.

  \nl

  For examples, modern Intel processors like Pentium ones were superpipelines
  processors with about \textbf{twenty} stages.

  \nl

  Of course, such pipelines are so complicated to study so we will concentrate
  on superpipelines with only \textbf{seven} stages.

  \nl

  This example is derived from the previously studied MIPS pipeline.

  \nl

  In this pipeline, we decided to divide the longest stages, of course
  the memory accesses.

  \nl

  We will try to compare the classical pipelines with deep pipelines.
\end{frame}

% 3)

\begin{frame}
  \frametitle{Stages}

  Let's see the pipeline used by this processor:

  \begin{center}
    \pgfuseimage{superpipeline-views}
  \end{center}
\end{frame}

% 4)

\begin{frame}
  \frametitle{Exercise}

  Let's draw the detailed superpipeline for the \textbf{Lw Rt, Immed(Rs)}
  instruction.
\end{frame}

% 5)

\begin{frame}
  \frametitle{Correction}

  \begin{center}
    \pgfuseimage{detailed-superpipeline}
  \end{center}
\end{frame}

% 6)

\begin{frame}
  \frametitle{Notices}

  \begin{itemize}[<+->]
    \item
      There may be memory accesses in the IFC1 and IFC2 stage. From this
      fact, the memory should be pipelined.

      \nl

      The same problem arises for the MEM1 and MEM2 stages.
    \item
      The next instruction address being computed in the DEC stage, there
      will be exactly \textbf{two} delay slots.
  \end{itemize}

  To conclude:

  \begin{itemize}[<+->]
    \item
      More time to perform memory access.
    \item
      Better balancing for the pipeline stages.
    \item
      But, we must pay these benefits: two delay slots and more stalls.
  \end{itemize}
\end{frame}

% 7)

\begin{frame}
  \frametitle{Exercise}

  Let's study the superpipeline and more precisely execution dependencies.

  \nl

  Try to find the superpipeline bypasses with an example for each bypass.
\end{frame}

% 8)

\begin{frame}
  \frametitle{Correction}

  The superpipeline is only composed of one additional bypass between
  the MEM1 and DEC stages.

  \begin{center}
    \pgfuseimage{superpipeline-bypasses}
  \end{center}
\end{frame}

% 9)

\begin{frame}[containsverbatim]
  \frametitle{Comparison}

  Let's compare the classical MIPS pipeline with the superpipeline.

  \nl

  We will call MIPS5 the classical pipeline and MIPS7 the superpipeline.

  \begin{verbatim}
    for (i = 0; i != size; i++)
      a[i] = a[i] + b
  \end{verbatim}

  In MIPS assembly:

  \begin{columns}

    \begin{column}{6cm}
      \begin{verbatim}
        Loop: Lw R6, 0(R5)
              Add R6, R6, R7
              Sw R6, 0(R5)
              Addi R5, R5, 4
              Bne R5, R8, Loop
              Nop
      \end{verbatim}
    \end{column}

    \begin{column}{6cm}
      \begin{verbatim}
        Loop: Lw R6, 0(R5)
              Add R6, R6, R7
              Sw R6, 0(R5)
              Addi R5, R5, 4
              Bne R5, R8, Loop
              Nop
              Nop
      \end{verbatim}
    \end{column}
  \end{columns}
\end{frame}

% 10)

\begin{frame}
  \frametitle{Exercise}

  Let's study the loop execution on the two processors considering that:

  \begin{itemize}
    \item
      The processor MIPS5 runs at 100 Mhz.
    \item
      The processor MIPS7 runs at 150 Mhz
  \end{itemize}
\end{frame}

% 11)

\begin{frame}
  \frametitle{Without Optimisation}

  One iteration takes:

  \begin{itemize}[<+->]
    \item
      \textbf{MIPS5}: $8~cycles / 100~Mhz = 8 / 100 * 10^{6} = 80~ns$
    \item
      \textbf{MIPS7}: $10~cycles / 150~Mhz = 10 / 150 * 10^{6} = 66~ns$
  \end{itemize}
\end{frame}

% 12)

\begin{frame}
  \frametitle{With Rescheduling Optimisation}

  One iteration takes:

  \begin{itemize}[<+->]
    \item
      \textbf{MIPS5}: $5~cycles / 100~Mhz = 5 / 100 * 10^{6} = 50~ns$
    \item
      \textbf{MIPS7}: $6~cycles / 150 Mhz = 6 / 150 * 10^{6} = 40~ns$
  \end{itemize}
\end{frame}

% 13)

\begin{frame}
  \frametitle{With Software Pipelining Optimisation}

  One iteration takes:

  \begin{itemize}[<+->]
    \item
      \textbf{MIPS5}: $5~cycles / 100~Mhz = 5 / 100 * 10^{6} = 50~ns$
    \item
      \textbf{MIPS7}: $5~cycles / 150~Mhz = 5 / 150 * 10^{6} = 33~ns$
  \end{itemize}
\end{frame}

% 13)

\begin{frame}
  \frametitle{With Loop Unrolling Optimisation}

  One \textbf{element} takes:

  \begin{itemize}[<+->]
    \item
      \textbf{MIPS5}: $4~cycles / 100~Mhz = 4 / 100 * 10^{6} = 40~ns$
    \item
      \textbf{MIPS7}: $4~cycles / 150~Mhz = 4 / 150 * 10^{6} = 26~ns$
  \end{itemize}
\end{frame}

% 14)

\begin{frame}
  \frametitle{Complex Instructions}

  We studied in this course the MIPS pipeline without taking care of the
  complex instructions like the multiplication and division.

  \nl

  We will now try to understand how these instructions are handled by the
  microprocessor and especially by the pipeline.
\end{frame}

% 15)

\begin{frame}
  \frametitle{New Strange Pipeline}

  The multiplication instruction is a very complex instruction. This type
  of instruction is about three times longer than a addition. In our example
  we will consider that the multiplication instruction takes exactly three
  cycles.

  \nl

  In the case of such an instruction the pipeline becomes strange because
  the last three stages are reserved to the multiplication.
\end{frame}

% 16)

\begin{frame}
  \frametitle{Complex Pipeline}

  Remember that the multiplication instruction stores the result in the
  two registers \textbf{HI} and \textbf{LO}.

  \nl

  Then, the programmer/compiler needs to use the instructions to access
  the result:

  \begin{itemize}
    \item
      \textbf{MFHI}: Move From HI.
    \item
      \textbf{MHLO}: Move From LO.
  \end{itemize}

  \begin{center}
    \pgfuseimage{multiplication-pipeline}
  \end{center}
\end{frame}

% 17)

\begin{frame}
  \frametitle{Multiplication Dependency}

  \begin{center}
    \pgfuseimage{multiplication-dependency}
  \end{center}
\end{frame}

% 18)

\begin{frame}
  \frametitle{Futur Works}

  \textbf{P = F / CPIu}

  \nl

  The pipeline technique permit us to multiply by N the frequency to build
  powerful microprocessors.

  \nl

  Then, we introduced the superpipelines with inherent properties:

  \begin{itemize}[<+->]
    \item
      More stalls.
    \item
      More delay slots.
    \item
      More bypasses cause more dependencies.
    \item
      Moreover, the multiplixers used to handle bypasses become complex
      and so the stage become longer.
  \end{itemize}

  At the end of the \textbf{90's}, researchers tried to reduce the CPI to
  reach: \textbf{CPI = 0.5 instr/cycle}
\end{frame}

% 19)

\begin{frame}
  \frametitle{Superscalar}

  Researchers tried to reach a lower CPI so to build microprocessors able
  to produce for example two instructions per cycle.

  \nl

  Moreover, this was not so complicated since this technique was almost
  used with complex instructions, for the multiplication for example.

  \nl

  These pipelines called \textbf{superscalar pipelines} will so look like:

  \begin{center}
    \pgfuseimage{superscalar-pipeline}
  \end{center}

  Nowadays, microprocessors have about four or six pipelines in parallel.
\end{frame}

% 20)

\begin{frame}
  \frametitle{Notices}

  You can notice some differencies or problems inherent to this new pipeline
  organisation:

  \begin{itemize}[<+->]
    \item
      The memory stage is critical since the memory is not able to handle
      two memory accesses in parallel.

      \nl

      The compiler will so have to avoid memory accesses in parallel.
    \item
      Moreover, the dependent instructions could not be executed at the
      same time.
  \end{itemize}
\end{frame}

% 21)

\begin{frame}[containsverbatim]
  \frametitle{Example}

  Let's take an example to understand this new pipeline.

  \begin{verbatim}
    for (i = 0; i != size; i++)
      v[i] = 2 * v[i];
  \end{verbatim}

  In MIPS assembly:

  \begin{verbatim}
    Loop: Lw R6, 0(R5)
          Sll R6, R6, 1
          Sw R6, 0(R5)
          Addiu R5, R5, 4
          Bne R5, R7, Loop
          Nop
          x
          y
  \end{verbatim}
\end{frame}

% 22)

\begin{frame}
  \frametitle{Pipelines}

  \begin{center}
    \pgfuseimage{superscalar-example}
  \end{center}
\end{frame}

% 22)

\begin{frame}
  \frametitle{Notices}

  This superscalar microprocessor is called:

  \nl

  \textbf{Superscalar Microprocessor With Ordered Execution}

  \nl

  meaning that an instruction can never be executed before a
  younger instruction.

  \nl

  Note that modern microprocessors like Intel ones are superscalar
  microprocessors with non ordered execution, with about four pipelines.

  \nl

  You can also notice that a \textbf{Prefetch Buffer} is used to store
  the next instructions to execute.

  \nl

  Moreover, the DEC stage now performs a \textit{+8} operation to compute
  the next instruction address.
\end{frame}

% 23)

\begin{frame}
  \frametitle{Problems}

  The IFC stage is a problem since the processor always has to access
  memory using aligned addresses.

  \nl

  The solution to this problem is to use an instruction buffer with in our
  case four slots.

  \nl

  Then the rule to fill in the instruction buffer is to wait for at least
  two unused slots. So memory accesses will always be aligned.
\end{frame}

% 24)

\begin{frame}[containsverbatim]
  \frametitle{Optimisation}

  Let's optimise the MIPS assembly code:

  \begin{verbatim}
    Loop: Lw R6, 0(R5)
          Addiu R5, R5, 4
          Sll R6, R6, 1
          Bne R5, R7, Loop
          Sw R6, -4(R5)
          x
          y
          z
  \end{verbatim}

  Be careful, a branch or jump instruction cannot be executed since its delay
  slots are not loaded in the instruction buffer.
\end{frame}

% 25)

\begin{frame}
  \frametitle{Correction}

  \begin{center}
    \pgfuseimage{superscalar-correction}
  \end{center}
\end{frame}

% 26)

\begin{frame}
  \frametitle{Conclusion}

  We can notice that the superscalar microprocessor is not better here
  than the classical MIPS pipeline since this loop takes \textbf{five}
  cycles.

  \nl

  Nevertheless, if we use the loop unrolling optimisation, this loop
  only takes \textbf{six} cycles for two iterations but memory access
  conflicts happen.
\end{frame}

% 27)

\begin{frame}
  \frametitle{Branch Prediction Overview}

  You can easily notice that branchs are cumbersome since everytime
  the prefetch instruction buffer is flushed so a cycle is lost.

  \nl

  To bypass this restriction researchers tried to find a way to predict
  branchs so to preload either the following instructions or the
  branched instructions.

  \nl

  There exists about three techniques to do so:

  \begin{enumerate}[<+->]
    \item
      \textbf{Fifty-Fifty}: a simple and stupid way consisting in a
      50\% choice: either the next instructions or the branched instructions.
    \item
      \textbf{Dynamic Branch Prediction}: this technique consists in
      memorising the last choice made. This binary information is stored
      with each instruction in a local memory called the \textbf{scory-board}.
    \item
      \textbf{Static Branch Prediction}: this technique uses the same
      method than the Dynamic Branch Prediction but in static. In other words,
      the information bit is set by the compiler or the programmer.
  \end{enumerate}
\end{frame}

% 28)

\begin{frame}
  \frametitle{Rules}

  Let's summerise the superscalar microprocessor rules:

  \begin{itemize}[<+->]
    \item
      Only one memory access at a time in the MEM stage.
    \item
      The instructions are launched in the DEC stage only if independent.
    \item
      The instructions are read by couple and at aligned addresses.
    \item
      A branch can be executed only if the delay slots are present in the
      instruction buffer.
  \end{itemize}
\end{frame}

% 29)

\begin{frame}[containsverbatim]
  \frametitle{Exercise}

  Let's take a little stupid subroutine:

  \begin{verbatim}
    for (i = 0; i != N; i++)
      {
        if (a[i] == b[i])
          c = 2 * a[i];
        else
          c = a[i] + b[i];

        s = s + c;
      }
  \end{verbatim}
\end{frame}

% 30)

\begin{frame}[containsverbatim]
  \frametitle{Assembly}

  The corresponding assembly code is:

  \begin{verbatim}
     Loop: Lw R7, 0(R5)
           Lw R8, 0(R6)
           Bne R7, R8, Else
           Sll R9, R7, 1
           Beq R0, R0, EndIf
     Else: Add R9, R7, R8
    EndIf: Add R2, R2, R9
           Addiu R5, R5, 4
           Addiu R6, R6, 4
           Addi R10, R10, -1
           Bne R10, R0, loop
  \end{verbatim}
\end{frame}

% 31)

\begin{frame}
  \frametitle{Work}

  \begin{enumerate}[<+->]
    \item
      Make this assembly code run on MIPS processors.
    \item
      Optimise this assembly code via rescheduling for superscalar
      microprocessors.
    \item
      Make the detailed pipelines for the \textit{If} case and then for
      the \textit{Else} case.
  \end{enumerate}
\end{frame}

%
% memory
%

\section{Memory}

% 1)

\begin{frame}
  \frametitle{Overview}

  We will study the Pi-Bus to understand how different entities can
  access memory sequentially.

  \nl

  Then we will concentrate on the memory and especially the different
  cache and policies used by the microprocessors.
\end{frame}

% 2)

\begin{frame}
  \frametitle{Pi-Bus}

  There are three types of objects:

  \begin{center}
    \pgfuseimage{pibus-overview}
  \end{center}

  \begin{enumerate}[<+->]
    \item
      \textbf{Masters}: microprocessors.
    \item
      \textbf{Slaves}: memory.
    \item
      \textbf{A referee}: BCU: Bus Control Unit
  \end{enumerate}
\end{frame}

% 3)

\begin{frame}
  \frametitle{Steps}

  The Pi-Bus steps are explained below:

  \begin{enumerate}[<+->]
    \item
      First, every master interested in acquiring the bus has to set
      its REQi bit.
    \item
      At the end of the cycle, the Bus Controller Unit chooses a master
      and set the appropriate GNTi bit.
    \item
      Then, the master sends the memory address on ADR with the desired
      memory size wanted: bytes, half-word, word on OPC.
    \item
      The referee then locates the slave corresponding the the desired
      memory location and notifies the slave to handle to request with
      SELj.
    \item
      Then the request is acknowleged on ACK and the data is transfered
      from the slave to the master DAT.
  \end{enumerate}
\end{frame}

% 4)

\begin{frame}
  \frametitle{Pi-Bus Communication}

  \begin{center}
    \pgfuseimage{pibus-communication}
  \end{center}
\end{frame}

% 5)

\begin{frame}
  \frametitle{\textbf{Sw} Instruction Example}

  \begin{center}
    \pgfuseimage{pibus-example}
  \end{center}
\end{frame}

% 6)

\begin{frame}
  \frametitle{Notices}

  Notice that the data cannot be transfered like this.

  \nl

  Before each transfer, the address must be specified to the slave.

  \nl

  Indeed, the Pi-Bus is in fact pipelined since it handles memory location
  while transfering the previous data.

  \nl

  You certainly also noticed that the refeering cycle was not used to
  transfer data, then the referee cycle is lost.

  \nl

  To bypass this limitation a default master is always chosen and a GRANT
  is sent to this master, so:

  \begin{itemize}[<+->]
    \item
      Either this master asks for the bus in this cycle so it already
      holds the bus and we won one cycle using a default master.
    \item
      Or it is not the case and in this case we \textit{lose} one
      cycle like before.
  \end{itemize}
\end{frame}

% 7)

\begin{frame}
  \frametitle{Bus Allocation Strategies}

  \textit{What is the algorithm used to acquire the bus?}
  \textit{How to avoid famine?}

  \begin{enumerate}[<+->]
    \item
      \textbf{Static Strategy}: an identifier is assigned to each master.
      This strategy is very dangerous introducing famine.
    \item
      \textbf{Dynamic Strategy}: give the bus in relation with the asking
      time. This strategy is hard to implement since the last master must
      keep a timestamp.
    \item
      \textbf{Dynamic Priorities Strategy}: first, a number is assigned
      to each master. Then the master just acquiring the bus, gets the
      lowest priority.

      \nl

      If two masters ask for the bus, the BCU just chooses the one with
      the highest priority.

      \nl

      Note that the default master in this strategy is simply the first
      in the list, the one with the highest priority.
  \end{enumerate}
\end{frame}

% 8)

\begin{frame}
  \frametitle{Memory}

  We will now study the memory components especially the different caches
  and cache policies.

  \nl

  Through this course we studied the MIPS microprocessor based on few
  requirements:

  \begin{enumerate}[<+->]
    \item
      The memory must provide:

      \begin{itemize}
	\item
	  one instruction per cycle.
	\item
	  one data per cycle.
      \end{itemize}
    \item
      A memory cycle will have to be as fast as the processor cycle.
    \item
      The memory address space must be 4 Gb.
    \item
      The memory must not be expensive.
  \end{enumerate}
\end{frame}

% 9)

\begin{frame}
  \frametitle{Memory Types}

  There exists different memory types or storage types:

  \begin{itemize}[<+->]
    \item
      \textbf{Static Memory}: This memory is very fast with an access time
      equal to the processor's one. But this type of memory is very
      expensive because built with silicium.

      \textit{Properties}: about Kb with an access time of 1 cycle.
    \item
      \textbf{Dynamic Memory}: This memory is seven times denser than
      the Static Memory. This memory is also built with silicium but it
      access time is about five to six times slower than the processor's
      one.

      \textit{Properties}: about Mb with an access time of 10 cycles.
    \item
      \textbf{Hard Drive}: This type of storage is mecanic so hard drives
      are about $10^{6}$ times slower than the processor.

      \textit{Properties}: about Gb with an access time of $10^{6}$ cycles.
    \item
      \textbf{Magnetic Tape}: Cheaper than hard drives because built with
      plastic.

      \textit{Properties}: about Tb with an access time of minutes.
  \end{itemize}
\end{frame}

% 10)

\begin{frame}
  \frametitle{Caches}

  You can notice that each memory type fulfills in a MIPS requirement but never
  all.

  \nl

  In fact, to fulfill the MIPS requirements MIPS designers had no choice than
  to use different memory types for different work.

  \nl

  The MIPS memories are organised hierarchically from the fastest to the
  slowest.
\end{frame}

% 11)

\begin{frame}
  \frametitle{Organisation}

  \begin{center}
    \pgfuseimage{memory-organisation}
  \end{center}
\end{frame}

% 12)

\begin{frame}
  \frametitle{Access Time}

  Let's try to evaluate the number of cycles that takes a memory access.

  \begin{center}
    \pgfuseimage{memory-access-time}
  \end{center}

  \nl

  \textbf{cycles per access}:

  $~~~~~= 1 * (4~Kb / 4~Gb) +$

  $~~~~~~~ 10 * ((4~Mb - 4~Kb) / 4~Gb) +$

  $~~~~~~~ 10^{6} * ((4~Gb - 4~Mb) / 4~Gb)$

  $~~~~~= 999 023$ cycles
\end{frame}

% 13)

\begin{frame}
  \frametitle{Spatial Locality}

  The previous result is theorically correct but we need to take care
  of the spatial locality.

  \nl

  Indeed, computer sciences programs respect the spatial locality meaning
  that memory accesses are never far from the previous one.

  \begin{center}
    \pgfuseimage{spatial-locality}
  \end{center}
\end{frame}

% 14)

\begin{frame}
  \frametitle{Access Time}

  Let's recompute the access time with these assumptions:

  \begin{itemize}
    \item
      Let's assume that 90\% of memory accesses are in cache and
      99.99999\% of memory accesses are in main memory.
    \item
      The Static Memory is loaded by 32 bytes (8 words) from the
      Dynamic Memory.
    \item
      The Dynamic Memory is loaded by 4 Kbytes from the Hard Drive.
  \end{itemize}

  \nl

  \textbf{cycles per access}:

  $~~~~~ = 1 * 0.90 +$

  $~~~~~~~ 8 * 10 * 0.10 * (1 - 10^{-7}) +$

  $~~~~~~~ 1024 * 10^{6} x 10^{-7}$

  $~~~~~ = 111$ cycles
\end{frame}

% 15)

\begin{frame}
  \frametitle{Review}

  Let's review for example an hard drive access.

  \nl

  We could decomposed an hard drive access into two very distinctive steps:

  \begin{itemize}[<+->]
    \item
      \textbf{A}: Move the reading head over the correct sector to
      perform the reading.
    \item
      \textbf{B}: Perform the reading.
  \end{itemize}

  The longest step is of course the move of the reading head because an hard
  drive is composed of mecanic components.

  \nl

  So to perform an hard drive reading of \textbf{one} word, the access time
  is: \textbf{A + B}.

  \nl

  Moreover, to perform an hard drive reading of \textbf{N} consecutive
  words, the access time is in fact: \textbf{A + N x B}.
\end{frame}

% 16)

\begin{frame}
  \frametitle{Reconsideration}

  Indeed, it is better to perform a huge amount of transfer to make reading
  head moves profitables.

  \nl

  This rule takes place for every memory which uses a bus because acquiring
  the bus takes time. For example for the Dynamic Memory, acquiring the
  bus takes \textbf{nine} cycles while transfering a data only takes
  \textbf{one} cycle.

  \nl

  So previous computations were false because we used a bad formula:
  \textbf{N x (A + B)}.

  \nl

  Moreover, the exact concrete probability that a data would be in cache
  is in fact \textbf{97\%} and more than \textbf{99.99999\%} for the main
  memory.
\end{frame}

% 17)

\begin{frame}
  \frametitle{Access Time}

  Let's consider that the bus is acquired in nine cycles, a data
  transfered via the bus in one cycle and the hard drive reading head is
  placed in about 999900 cycles while a data is transfered in about
  100 cycles.

  \nl

  \textbf{cycles per access}:

  $~~~~~ = 1 x 0.97 +$

  $~~~~~~~ (9 + (8 x 1)) x 0.03 x (1 - 10^-7) +$

  $~~~~~~~ (999900 + (1024 x 100)) x 10^-7$

  $~~~~~ = 1.62$ cycles
\end{frame}

% 18)

\begin{frame}
  \frametitle{Real World}

  \begin{center}
    \pgfuseimage{cache-hierarchy}
  \end{center}

  \begin{itemize}
    \item
      Spatial Locality.
    \item
      Transfers optimised for data packets.
  \end{itemize}
\end{frame}

% 19)

\begin{frame}
  \frametitle{Caches Overview}

  There exists two different cache types:

  \begin{itemize}[<+->]
    \item
      \textbf{Unified Caches}.
    \item
      \textbf{Specific Caches}.
  \end{itemize}
\end{frame}

% 20)

\begin{frame}
  \frametitle{Unified Caches}

  These types of caches managed instruction like data in the same cache.

  \nl

  The properties are:

  \begin{itemize}[<+->]
    \item
      Complex because must respond to multiple requests: instruction and data
      because there are two memory accesses per cycles, in the IFC and MEM
      stages.
    \item
      Must be very fast to manage two requests.
    \item
      But, these caches can optimise the space management since it may be
      few instructions and many data, the cache can be filled in with
      90\% of data.
  \end{itemize}

  We will not study these caches because too complex.
\end{frame}

% 21)

\begin{frame}
  \frametitle{Specific Caches}

  These types of cache are simpler to build than Unified Caches.

  \nl

  The only problem with these caches is the space management which
  can be extremely limited in a special memory use: for example many
  instruction with few data.

  \nl

  But, in this type of caches, the two different subcaches can use
  a specific Cache Policy.

  \nl

  These caches simply distinguish the instruction from the data and are
  in fact composed of two caches:

  \begin{itemize}[<+->]
    \item
      Instruction Cache.
    \item
      Data Cache.
  \end{itemize}
\end{frame}

% 22)

\begin{frame}
  \frametitle{Cache Overview}

  Let's first study the simplest cache, the Instruction Cache because
  instructions are never overwritten.

  \begin{center}
    \pgfuseimage{cache-overview}
  \end{center}
\end{frame}

% 23)

\begin{frame}
  \frametitle{Cache Manager}

  We will call the Cache Manager the Master on the Pi-Bus which communicates
  with the memories.

  \begin{center}
    \pgfuseimage{pibus-cache}
  \end{center}
\end{frame}

% 24)

\begin{frame}
  \frametitle{Cache Policies}

  The question is: \textit{Where is the block located in the cache?}

  \nl

  There exists three different cache policies:

  \begin{enumerate}[<+->]
    \item
      \textbf{Direct Mapping}: one block can only be located in one
      slot in the cache at a time.
    \item
      \textbf{Set Associative}: one block can be located in a set of
      slots.
    \item
      \textbf{Full Associative}: one block can be located everywhere
      in the cache.
  \end{enumerate}
\end{frame}

% 25)

\begin{frame}
  \frametitle{Direct Mapping Overview}

  The main memory is divided in partitions. Then a cache slot is
  logically assigned to each partition.

  \nl

  A block can only be stored in the cache slot of its partition.
\end{frame}

% 26)

\begin{frame}
  \frametitle{Natural Partitionning}

  \begin{center}
    \pgfuseimage{natural-partitionning}
  \end{center}
\end{frame}

% 27)

\begin{frame}
  \frametitle{Notices}

  This partitionning is absolutely critical since it does not respect
  the Spatial Locality Rule.

  \nl

  The best partitionning would be in columns to keep away blocks with
  Spatial Locality.

  \nl

  With it, it would be possible to keep in cache sequential blocks.
\end{frame}

% 28)

\begin{frame}
  \frametitle{Example}

  An example with \textbf{64} blocks of \textbf{16} bytes each. We want
  to access the \textbf{fourth} byte of the block \textbf{9}.

  \begin{center}
    \pgfuseimage{cache-example}
  \end{center}
\end{frame}

% 29)

\begin{frame}
  \frametitle{Cache Internals}

  \begin{center}
    \pgfuseimage{cache-internals}
  \end{center}
\end{frame}

% 30)

\begin{frame}
  \frametitle{Notices}

  Let's notice that a directory entry like a data entry can take exactly
  \textbf{nine} values, in the case of our example.

  \begin{itemize}[<+->]
    \item
      The eight different possible blocks.
    \item
      Or an empty entry meaning that this entry is not used, yet.
  \end{itemize}

  So, the Block Tag is not enough.

  \nl

  The cache needs another information to know if the entry is used.
\end{frame}

% 31)

\begin{frame}
  \frametitle{Present Bit}

  We will so add a \textbf{Present Bit} in every Cache Directory Entry.

  \nl

  Moreover, another test has to be done to check the present bit.
\end{frame}

% 32)

\begin{frame}[containsverbatim]
  \frametitle{Performances}

  Researchers perform benchmarks on microprocessor's caches
  computing the Miss Rate.

  \nl

  $Miss~Rate = Number~of~Misses / Number~of~Accesses$

  \nl

  The Miss Rate reflects the cache performances.

  \nl

  Reloading the cache takes time, about \textbf{50} cycles.

  \nl

  Conclusion, the Direct Mapping Policy is powerful, but not enough because
  too restrictive on the block placement in cache. If data operations
  are made on blocks of a same set, the cache will always load then unload
  a block of the cache.
\end{frame}

% 33)

\begin{frame}
  \frametitle{Set Associative Overview}

  The Set Associative Policy is used to merge, associate cache entries.

  \begin{center}
    \pgfuseimage{set-associative-overview}
  \end{center}
\end{frame}

% 34)

\begin{frame}
  \frametitle{Cache Internals}

  \begin{center}
    \pgfuseimage{set-associative-internals}
  \end{center}
\end{frame}

% 35)

\begin{frame}
  \frametitle{Cache Replacement Policies}

  The remaining question is: \textit{What block to be replaced by?}

  \nl

  There exists three interesting algorithms:

  \begin{itemize}[<+->]
    \item
      FIFO.
    \item
      LIFO.
    \item
      LRU.
  \end{itemize}
\end{frame}

% 36)

\begin{frame}
  \frametitle{Temporal Locality}

  Of course, the algorithm used in the cache is a pseudo LRU.

  \nl

  Indeed, the Temporal Locality is a fact and the microprocessor needs
  an algorithm which keeps in cache the blocks which will be used in a near
  futur.

  \nl

  Be careful, the Temporal Locality is less important than the Spatial
  Locality so cache implementations should not based everything on
  Temporal Locality.

  \begin{center}
    \pgfuseimage{temporal-locality}
  \end{center}
\end{frame}

% 37)

\begin{frame}
  \frametitle{Pseudo LRU}

  The pseudo LRU is a lightweigth LRU implemented through only one bit
  per block.

  \nl

  The algorithm works as follows.

  \nl

  Consider a binary search tree for the items in question. Each node of
  the tree has a one-bit flag denoting ``go left to find a pseudo-LRU element''
  or ``go right to find a pseudo-LRU element''. To find a pseudo-LRU element,
  traverse the tree according to the values of the flags. To update the
  tree with an access to an item N, traverse the tree to find N and, during
  the traversal, set the node flags to denote the direction that is opposite
  to the direction taken.
\end{frame}

% 38)

\begin{frame}
  \frametitle{Full Associative Overview}

  The Full Associative Cache Policy is in fact a Set Associative Policy
  with the maximum degree.

  \nl

  Moreover, the Direct Mapping Policy was simply a Set Associative of
  Degree One.

  \nl

  In our example, a Full Associative Cache would be the same cache than a
  Set Associative Cache with a Degree Eight.

  \nl

  Of courses these types of cache are theoritical since the
  hardware become too complex with many multiplexers slowing down
  the microprocessor CPI so the frequency.
\end{frame}

% 39)

\begin{frame}
  \frametitle{Implementations}

  The caches are in fact implemented through the three different policies
  below:

  \begin{itemize}[<+->]
    \item
      Direct Mapping.
    \item
      Set Associative Degree Two.
    \item
      Set Associative Degree Four.
  \end{itemize}

  Then every cache with a more complex implementation is usually avoided
  since the multiplexers become too complex.

  \nl

  Of course the Full Associative Policy is never implemented.
\end{frame}

% 40)

\begin{frame}
  \frametitle{Data Caches Overview}

  We just studied the different cache policies on the simplest caches:
  the instruction caches.

  \nl

  These caches are simpler to understand because the instructions are
  never overwritten.

  \nl

  We will now look at the data caches and the different Write Policies
  which describe the way to update the data in cache, in main memory etc..
\end{frame}

% 41)

\begin{frame}
  \frametitle{Write Policies Overview}

  There exists two different Write Policies:

  \begin{itemize}[<+->]
    \item
      \textbf{Write Through}: Simpler caches. Every writing goes to the
      memory with taking care of updating the cache.

      \textit{Properties}: Slow writings.
    \item
      \textbf{Write Back}: More complex caches. Every writing just updates
      the cache entry if any. Otherwise, the block is loaded in cache
      and updated here.

      \textit{Properties}: Inconsistent data between the cache and the memory.
      This property is very cumbersome for multiprocessors. Nevertheless this
      Write Policy is faster than the other.
  \end{itemize}
\end{frame}

% 42)

\begin{frame}
  \frametitle{Dirty Bit}

  In the Write Back Policy, a block in cache is updated each time this
  block is overwritten.

  \nl

  Nevertheless, this block can be replaced by another. In this case, the
  block should be rewrite into memory.

  \nl

  To know if a block has to be rewrite into memory a \textbf{Dirty Bit} is
  added to every cache directory entry.

  \nl

  Every time a block is replaced, the cache manager looks at the Dirty Bit.
  If set, the block is rewritten into memory.
\end{frame}

% 43)

\begin{frame}
  \frametitle{Multiprocessors}

  Let's talk about multiprocessors inherent problems and solutions.

  \nl

  Consider two microprocessors \textbf{P0} and \textbf{P1} and a Write
  Back Cache Policy.

  \nl

  P0 and P1 read a block \textbf{B}, so this block is loaded in their
  own cache.

  \nl

  Then, P0 writes into this block updating the cache entry corresponding to
  the block B.

  \nl

  At this point, the two microprocessors P0 and P1 have different
  views of the block B.
\end{frame}

% 44)

\begin{frame}
  \frametitle{Automata}

  To bypass this problem an automata was introduced to probe the bus
  waiting for Invalidation Signals.

  \nl

  This automata is called the \textbf{Snoopy}, from the famous dog.

  \nl

  Invalidation Signals are sent on the Snoopy Bus.
\end{frame}

% 45)

\begin{frame}
  \frametitle{Snoopy}

  \begin{center}
    \pgfuseimage{snoopy}
  \end{center}
\end{frame}

% 46)

\begin{frame}
  \frametitle{Modified Bit}

  Of course, sending an Invalidation Signal takes time because the bus needs
  to be acquired first.

  \nl

  To minimise the number of Invalidation Signals sent a \textbf{Modified Bit}
  was added to every cache directory entry.

  \nl

  Then the following automata is used for every writing:

  \begin{enumerate}[<+->]
    \item
      If M is equal to zero, then an Invalidation Signal is sent and the
      Modified Bit M is set to one.
    \item
      If M is equal to one, then nothing is done because every processor
      already know that this block is dirty.
  \end{enumerate}
\end{frame}

% 47)

\begin{frame}
  \frametitle{Synchronisation}

  The remaining question is: \textit{How to make processors synchronised?}

  \nl

  When P1 will want to access the block B, this will be from the main memory.

  \nl

  Of course, when a microprocessor receive an Invalidation Signal, it
  invalidates, if necessary, the cache entry corresponding to the block.

  \nl

  Then the following steps take place:

  \begin{enumerate}[<+->]
    \item
      The bus access will be refused. A \textit{RETRACT} signal is sent
      to the master.
    \item
      The P0 snoopy detects this access and then updates memory with
      its cache entry taking care of reinitialising the Dirty Bit and Modified
      Bit.
    \item
      Then P1 can retry its reading because the block in memory is now
      consistent.
  \end{enumerate}
\end{frame}

% 48)

\begin{frame}
  \frametitle{Comparison}

  The Write Back Policy is a powerful solution but is very expensive
  due to the Snoopy and the inconsistent memory.

  \nl

  Then researchers decided to try to optimise the Write Through Policy
  since it was simpler.
\end{frame}

% 49)

\begin{frame}
  \frametitle{Store Buffer}

  The Write Through Policy main problem is the time it takes to write
  data into memory.

  \nl

  Indeed, during the memory writing the processor waits for the end of this
  operation, all this time is wasted introducing stalls into the pipeline.

  \nl

  So a Store Buffer was introduced to optimise the writings.

  \nl

  Everytime a block is overwritten, the block is pushed in the Store Buffer.

  \nl

  Then the cache automata's role is now to get a block from the Store Buffer
  and to write it into memory.

  \nl

  During this time, the microprocessor can return to its work.
\end{frame}

% 50)

\begin{frame}
  \frametitle{Cache Automata}

  \begin{center}
    \pgfuseimage{cache-automata}
  \end{center}
\end{frame}

% 51)

\begin{frame}
  \frametitle{Conclusion}

  The different Cache Policies can be easily summarise:

  \begin{itemize}[<+->]
    \item
      Write Through.

      \begin{itemize}
	\item
	  With Store Buffer.
	\item
	  Without Store Buffer. The microprocessor is blocked until the
	  block is written back into memory.
      \end{itemize}

    \item
      Write Back.

      \begin{itemize}
	\item
	  The block in cache is simply updated.
      \end{itemize}
  \end{itemize}

  Note, that in the case that the written block is not present in cache,
  this block is loaded into the cache from the respect of the Spatial
  Locality.
\end{frame}

% 52)

\begin{frame}
  \frametitle{Exercise}

  Consider a MIPS microprocessor with a five stages pipeline. We want to
  study to memory performances influence on the entire microprocessor
  performances.

  \nl

  With a perfect memory system (readings and writings in one cycle),
  the CPI is equal to \textbf{1.1} cycle per instruction.

  \nl

  Load instructions represent \textbf{20\%} of the executed instructions.
  The store instructions represent \textbf{10\%} of them.

  \nl

  The physical memory is composed of \textbf{64} Mbytes. The memory
  is addressed by byte. The memory bus has a \textbf{4} bytes bandwith.

  \nl

  The bus cycle time is equal to \textbf{4} microprocessor cycles.

  \nl

  In the concrete system, a memory reading needs \textbf{10} cycles while
  a writing needs \textbf{4} cycles.
\end{frame}

% 53)

\begin{frame}
  \frametitle{Question}

  \textit{From the perfect system, what is the increase of the
    CPI called RP due to writings?}

  \nl

  \textit{From the perfect system, what is the increase of the
    CPI called WP due to writings?}
\end{frame}

% 54)

\begin{frame}
  \frametitle{Reading Properties}

  On a perfect system a reading takes one cycle while in a concrete system
  a reading takes 10 cycles. The reading penalty is so \textbf{9} cycles
  per instruction.

  \nl

  Note that we can classify instructions into three categories:

  \begin{itemize}[<+->]
    \item
      \textbf{Load Instructions}
    \item
      \textbf{Store Instructions}
    \item
      \textbf{Others}
  \end{itemize}

  Moreover, we can easily compute the number of reading instructions and
  their probabilities:

  \begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|}
    \hline

    \textbf{Instruction Type} & \textbf{Probability} &
    \textbf{Reading Occurences} \\

    \hline

    Load Instructions & 0.2 & IFC and MEM \\

    \hline

    Store Instructions & 0.1 & IFC \\

    \hline

    Others & 0.7 & IFC \\

    \hline
  \end{tabular}
\end{frame}

% 55)

\begin{frame}
  \frametitle{Reading Penalty}

  Finally, the reading penalty is equal to:

  \nl

  \textbf{RP}:

  $~~~~~= penalty / reading * reading~number / instruction$

  $~~~~~= 9 * (0.2 * 2 + 0.1 * 1 + 0.7 * 1)$

  $~~~~~= 9 * 1.2$

  $~~~~~= 10.8$ cycles
\end{frame}

% 56)

\begin{frame}
  \frametitle{Write Penalty}

  The same calculations come for writings.

  \nl

  On a perfect system a writing takes one cycle while in a concrete system
  a writing takes 4 cycles.

  \nl

  The writing penalty is so \textbf{3} cycles per instruction.

  \begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|}
    \hline

    \textbf{Instruction Type} & \textbf{Probability} &
    \textbf{Writing Occurences} \\

    \hline

    Load Instructions & 0.2 & \\

    \hline

    Store Instructions & 0.1 & MEM \\

    \hline

    Others & 0.7 & \\

    \hline
  \end{tabular}

  Finally, the writing penalty is equal to:

  \nl

  \textbf{WP}:

  $~~~~~= penalty / writing * writing~number / instruction$

  $~~~~~= 3 * (0.2 * 0 + 0.1 * 1 + 0.7 * 0)$

  $~~~~~= 3 * 0.1$

  $~~~~~= 0.3$ cycles
\end{frame}

% 57)

\begin{frame}
  \frametitle{Question}

  \textit{What is the average number of cycles per instruction?}
\end{frame}

% 58)

\begin{frame}
  \frametitle{Solution}

  The average CPI is now equal to:

  \nl

  \textbf{CPI}:

  $~~~~~= 1.1 + 10.8 + 0.3$

  $~~~~~= 12.2$ cycles
\end{frame}

% 59)

\begin{frame}
  \frametitle{Exercise}

  To optimise memory accesses, we introduce a two levels cache. The
  cache policy used is set associative degree two. The cache is a
  unified cache of 16 Kbytes size.

  \nl

  The access time is one cycle. The block size is 16 bytes.

  \nl

  The write strategy is Write Through without updating the cache whether
  the data is not loaded in cache. There is no Store Buffer for writings.
\end{frame}

% 60)

\begin{frame}
  \frametitle{Question}

  \textit{Describe the physical address from the cache point of view,
    precising every field's size?}
\end{frame}

% 61)

\begin{frame}
  \frametitle{Solution}

  The number of sets in the cache is:

  \nl

  \textbf{sets number}:

  $~~~~~= cache~size / block~size / associative~degree$

  $~~~~~= 16~Kb / 16~b / 2$

  $~~~~~= 512$ sets

  \nl

  Note that with a 64 Mbytes memory composed of blocks of 16 bytes size,
  there are exactly 4 Mblocks or $2^{22}$ blocks of physical memory.

  \nl

  So, the physical address format from the cache point of view is:

  \begin{enumerate}
    \item
      \textbf{Tag}: 15 bits ($22 - 9$) because there are 4 Mblocks of
      physical memory.
    \item
      \textbf{Set}: 9 bits because there are 512 sets.
    \item
      \textbf{Offset}: 4 bits because a block has a 16 bytes size.
  \end{enumerate}
\end{frame}

% 62)

\begin{frame}
  \frametitle{Question}

  \textit{Describe the cache internals?}
\end{frame}

% 63)

\begin{frame}
  \frametitle{Question}

  \textit{What is the usefulness of the set associative cache policy
    of degree two?}
\end{frame}

% 64)

\begin{frame}
  \frametitle{Solution}

  To respect the Temporal Locality.
\end{frame}

% 65)

\begin{frame}
  \frametitle{Exercise}

  Consider that the Miss Rate is about \textbf{5\%}.

  \nl

  For every Miss, the microprocessor waits the end of the block
  load in the cache.

  \nl

  We want to compute the average number of cycles per instruction.

  \nl

  Remember that a reading takes \textbf{10} cycles. In these 10 cycles,
  \textbf{4} cycles are reserved for the address transfer and \textbf{4}
  cycles are reserved for the data transfer. Each of these transfer is
  a 4 bytes transfer.

  \nl

  Then the \textbf{2} remaining cycles are reserved by the bus and the memory
  to perform internal operations.
\end{frame}

% 66)

\begin{frame}
  \frametitle{Question}

  \textit{From the microprocessor point of view, what is the length of
    a memory reading in the case of a cache hit?}

  \nl

  \textit{From the microprocessor point of view, what is the length of
    a memory reading in the case of a cache miss?}
\end{frame}

% 67)

\begin{frame}
  \frametitle{Solution}

  In the case of a hit, the reading length is one cycle.

  \nl

  In the case of a miss, the block must be loaded in cache. Once the block
  entirely loaded the microprocessor can continue. This policy is not
  optimised since the microprocessor could only wait for the desired byte
  or bytes.

  \nl

  Remember that the bus is pipelined. The transfer will look like that:

  \begin{center}
    \pgfuseimage{exercise-bus}
  \end{center}

  The reading length in a miss case is so:

  \nl

  $1 + (4 + (2 + 4) x 4) + 1 = 30$ cycles
\end{frame}

% 68)

\begin{frame}
  \frametitle{Question}

  \textit{From the perfect system, what are the CPI increases for
    readings and writings?}
\end{frame}

% 69)

\begin{frame}
  \frametitle{Write Penalty}

  The Write Penalty is the same than before since there is no Store
  Buffer for writings.

  \nl

  The Write Penalty is so the same than if there is no Store Buffer:
  \textbf{0.3} cycles per writing.
\end{frame}

% 70)

\begin{frame}[containsverbatim]
  \frametitle{Read Penalty}

  There are two cases:

  \begin{itemize}
    \item
      \textbf{Hit}: 0 cycles because the data is in cache so the access time
      is one cycle so no penalty.
    \item
      \textbf{Miss}: $(30 - 1) x 1.2 = 34.8$ penalty cycles.
  \end{itemize}

  To conclude, the reading penalty is equal to:

  \nl

  \textbf{RP}:

  $~~~~~= 0 * 0.95 + 34.8 * 0.05$

  $~~~~~= 1.740$ cycles per instruction
\end{frame}

% 71)

\begin{frame}
  \frametitle{Question}

  \textit{What is the average number of cycles per instruction?}
\end{frame}

% 72)

\begin{frame}
  \frametitle{Solution}

  The CPI is now:

  \nl

  \textbf{CPI}:

  $~~~~~= 1.1 + 0.3 + 1.740$

  $~~~~~= 3.140$ cycles
\end{frame}

%
% bibliography
%

\section{Bibliography}

\begin{thebibliography}{4}

  \bibitem{R2000-R3000}
    MIPS RISC Architecture
    \newblock This book includes R2000/R2010 and R3000/R3010
    \newblock By Gerry Kane

  \bibitem{R4000}
    MIPS RISC Architecture
    \newblock This book includes the R2000, R3000, R6000 ad the new R4000
    \newblock By Gerry Kane and Joe Heinrich

  \bibitem{Computer Organization}
    Computer Organization and Design. The Hardware/Software Interface
    \newblock This book about computer design in general, and RISC
    in particular, takes its examples directly from the MIPS architecture
    \newblock Patterson and Hennessy

  \bibitem{MIPS Run}
    See Mips Run
    \newblock The definitive book on the MIPS architecture
    \newblock Dominic Sweetman

\end{thebibliography}

\end{document}
