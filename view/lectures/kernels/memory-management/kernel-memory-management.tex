%%
%% licence       kaneton licence
%%
%% project       kaneton
%%
%% file          /home/buckman/kaneton/view/lectures/kernels/memory-management/kernel-memory-management.tex
%%

%
% template
%

\input{../../../templates/lecture.tex}

%
% title
%

\title{Kernels - Memory management}

%
% authors
%

\author
{
  Matthieu~Bucchianeri and Renaud~Voltz\inst{1}
}

%
% figures
%

%
%\pgfdeclareimage[interpolate=true,width=188pt,height=97pt]
%                {sample}
%		{figures/sample}

% virtual memory

\pgfdeclareimage[interpolate=true,width=135pt,height=180pt]
                {vmem-overlap}
		{figures/vmem-overlap}
\pgfdeclareimage[interpolate=true,width=135pt,height=180pt]
                {vmem-contiguous}
		{figures/vmem-contiguous}
\pgfdeclareimage[interpolate=true,width=135pt,height=180pt]
                {vmem-sharing}
		{figures/vmem-sharing}

% bank

\pgfdeclareimage[interpolate=true,width=153pt,height=180pt]
                {68hc11-bank}
		{figures/68hc11-bank}

% seg

\pgfdeclareimage[interpolate=true,width=204pt,height=120pt]
                {ia32-seg}
		{figures/ia32-seg}
\pgfdeclareimage[interpolate=true,width=198pt,height=180pt]
                {ia32-multiseg}
		{figures/ia32-multiseg}
\pgfdeclareimage[interpolate=true,width=90pt,height=180pt]
                {mips-as}
		{figures/mips-as}

% paging

\pgfdeclareimage[interpolate=true,width=233pt,height=100pt]
                {paging-step1}
		{figures/paging-step1}
\pgfdeclareimage[interpolate=true,width=233pt,height=100pt]
                {paging-step2}
		{figures/paging-step2}
\pgfdeclareimage[interpolate=true,width=233pt,height=100pt]
                {paging-step3}
		{figures/paging-step3}
\pgfdeclareimage[interpolate=true,width=233pt,height=100pt]
                {paging-error-step1}
		{figures/paging-error-step1}
\pgfdeclareimage[interpolate=true,width=233pt,height=100pt]
                {paging-error-step2}
		{figures/paging-error-step2}
\pgfdeclareimage[interpolate=true,width=233pt,height=100pt]
                {paging-miss-step1}
		{figures/paging-miss-step1}
\pgfdeclareimage[interpolate=true,width=233pt,height=100pt]
                {paging-miss-step2}
		{figures/paging-miss-step2}

% tlb

\pgfdeclareimage[interpolate=true,width=187pt,height=100pt]
                {detailed-tlb}
		{figures/detailed-tlb}
\pgfdeclareimage[interpolate=true,width=270pt,height=120pt]
                {mips-tlb}
		{figures/mips-tlb}
\pgfdeclareimage[interpolate=true,width=268pt,height=100pt]
                {direct-mapped}
		{figures/direct-mapped}
\pgfdeclareimage[interpolate=true,width=196pt,height=180pt]
                {tlb-coherency-step1}
		{figures/tlb-coherency-step1}
\pgfdeclareimage[interpolate=true,width=196pt,height=180pt]
                {tlb-coherency-step2}
		{figures/tlb-coherency-step2}

% smp

\pgfdeclareimage[interpolate=true,width=210pt,height=180pt]
                {smp-coherency-step1}
		{figures/smp-coherency-step1}
\pgfdeclareimage[interpolate=true,width=210pt,height=180pt]
                {smp-coherency-step2}
		{figures/smp-coherency-step2}
\pgfdeclareimage[interpolate=true,width=210pt,height=180pt]
                {smp-coherency-step3}
		{figures/smp-coherency-step3}

% ia32-mmu

\pgfdeclareimage[interpolate=true,width=221pt,height=180pt]
                {ia32-mmu}
		{figures/ia32-mmu}
\pgfdeclareimage[interpolate=true,width=154pt,height=180pt]
                {pd-pt}
		{figures/pd-pt}

% slab

\pgfdeclareimage[interpolate=true,width=280pt,height=160pt]
                {slabs}
		{figures/slabs}
\pgfdeclareimage[interpolate=true,width=243pt,height=160pt]
                {slab-allocator}
		{figures/slab-allocator}

% stack

\pgfdeclareimage[interpolate=true,width=70pt,height=160pt]
                {stack-exp-step1}
		{figures/stack-exp-step1}
\pgfdeclareimage[interpolate=true,width=70pt,height=160pt]
                {stack-exp-step2}
		{figures/stack-exp-step2}
\pgfdeclareimage[interpolate=true,width=70pt,height=160pt]
                {stack-exp-step3}
		{figures/stack-exp-step3}

% alloc-on-demand

\pgfdeclareimage[interpolate=true,width=60pt,height=160pt]
                {alloc-demand-step1}
		{figures/alloc-demand-step1}
\pgfdeclareimage[interpolate=true,width=60pt,height=160pt]
                {alloc-demand-step2}
		{figures/alloc-demand-step2}
\pgfdeclareimage[interpolate=true,width=60pt,height=160pt]
                {alloc-demand-step3}
		{figures/alloc-demand-step3}


% copy-on-write

\pgfdeclareimage[interpolate=true,width=120pt,height=160pt]
                {cow-step1}
		{figures/cow-step1}
\pgfdeclareimage[interpolate=true,width=120pt,height=160pt]
                {cow-step2}
		{figures/cow-step2}
\pgfdeclareimage[interpolate=true,width=120pt,height=160pt]
                {cow-step3}
		{figures/cow-step3}
\pgfdeclareimage[interpolate=true,width=120pt,height=160pt]
                {cow-step4}
		{figures/cow-step4}

%
% document
%

\begin{document}

%
% title frame
%

\begin{frame}
  \titlepage

  \begin{center}
    \logos
  \end{center}
\end{frame}

%
% outline frame
%

\begin{frame}
  \frametitle{Outline}
  \tableofcontents
\end{frame}

% -)

\begin{frame}
  \frametitle{Map of the lessons}

  In the first lesson, we will focus on the basics of memory
  management in a kernel in order to provide the main system calls
  (\emph{mmap}).

  \begin{itemize}
  \item
    Bank switching
  \item
    Segmentation
  \item
    Paging
  \end{itemize}

  \-

  In the second lesson, we will deal with higher level algorithms of
  allocation and with swapping.

  \begin{itemize}
  \item
    User memory allocation
  \item
    Swapping
  \end{itemize}

\end{frame}

%
% physical memory & virtual memory
%

\section{Physical memory \& virtual memory}

%-)

\begin{frame}
  \frametitle{Definition of address space, virtual memory and MMU}

  An address space is a set of memory area refering to the physical
  RAM or to I/O.

  \-

  Address spaces are protected, meaning that only the owner of an
  address space can access it.

  \-

  The concept of virtual memory is to translated the addresses given
  in a program into physical addresses in RAM.

  \-

  The MMU (\emph{Memory Management Unit}) is a part of the CPU in
  charge of translating logical addresses (from programs) into
  physical addresses given a few rules.

\end{frame}

%-)

\begin{frame}
  \frametitle{Protection}

  \begin{center}
   \pgfuseimage{vmem-overlap}
  \end{center}

\end{frame}

%-)

\begin{frame}
  \frametitle{Contiguous allocation}

  \begin{center}
   \pgfuseimage{vmem-contiguous}
  \end{center}

\end{frame}

%-)

\begin{frame}
  \frametitle{Region sharing}

  \begin{center}
   \pgfuseimage{vmem-sharing}
  \end{center}

\end{frame}

%
% bank switching
%

\section{Bank switching}

%-)

\begin{frame}
  \frametitle{Bank switching or MMU of poor}

  The multiple bank technique is the simplest mechanism to implement
  virtual memory.

  \-

  The technique is also used to expand memory size with
  microcontrollers or 8-bit microprocessors with 16-bit address bus.

\end{frame}

%-)

\begin{frame}
  \frametitle{68HC11 example}

  \begin{center}
   \pgfuseimage{68hc11-bank}
  \end{center}

\end{frame}

%-)

\begin{frame}
  \frametitle{Limitations}

  \begin{itemize}
  \item
    Limited in size (for example, only 16 Kb by address space)
  \item
    More complicated programming (managing the PPAGE register),
    special compilers
  \item
    Not all possibilities we want: contiguous mapping or
    sharing\ldots{} are possible but not trivial
  \end{itemize}

  \-

  Bank switching is still used with small microprocessors, but modern
  microprocessors offer better mechanisms.

\end{frame}

%
% segmentation
%

\section{Segmentation}

% -)

\begin{frame}
  \frametitle{Overview}

  Segmentation is a mechanism where the CPU or the kernel defines
  multiple area in memory with specific privileges.

  \-

  Most MMU provides fixed segments, while more complicated one let the
  kernel filling structures to declare the segments.

\end{frame}

% -)

\begin{frame}
  \frametitle{MIPS example}

  \begin{center}
   \pgfuseimage{mips-as}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{IA-32 example, real mode}

  Real mode on Intel microprocessors divides the 1 Mb address space in
  16 segments of 64 Kb each. A segment is selected using the segment
  selector registers (the 4 upper bits).

  \-

  Even if this is called segmentation, no protection is ensured.

\end{frame}

% -)

\begin{frame}
  \frametitle{IA-32 example, protected mode}

  On IA-32 in protected mode, segments can be defined by the
  kernel. Up to 16000 segments can be simultaneously defined.

  \-

  A table, called the GDT, containts records for each segment with:

  \begin{itemize}
  \item
    Base address
  \item
    Limit
  \item
    Privilege level
  \end{itemize}

  \-

  Selecting a segment is done with segment selector registers, filled
  with the index of the required segment in the GDT.

\end{frame}

% -)

\begin{frame}
  \frametitle{IA-32 example, protected mode}

  \begin{center}
   \pgfuseimage{ia32-seg}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{IA-32 example, protected mode}

  \begin{center}
   \pgfuseimage{ia32-multiseg}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Disabling segmentation}

  Many kernels do not use segmentation and prefer paging. But
  disabling segmentation is not possible.

  \-

  Instead, the kernel setup a flat model (or protected flat model),
  meaning that it creates one segment (per privilege level) overlaying
  all the physical memory.

  \-

  In the case of IA-32, two segments are required because permissions
  can be set either on RW or RX, so we must produce a combination RWX.

\end{frame}

%
% paging
%

\section{Paging}

% -)

\begin{frame}
  \frametitle{Overview}

  Paging is the concept of dividing the physical memory into chunk of
  constant size (for example 4 Kb) called page frames.

  \-

  These page frames can be mapped precisely into address spaces. Thus,
  a virtual address used in a program is translated by the MMU info a
  physical address.

  \-

  With this technique, protection, contiguous allocation and sharing
  can be done easily. But unlike with segmentation or bank switching,
  the MMU is more complex to build and to program, and the translation
  are slower.

  \-

  In addition, we will see later how paging makes easy to implement
  swapping technique.

\end{frame}

% -)

\begin{frame}
  \frametitle{Translation Lookaside Buffers}

  \begin{itemize}
    \item
    This translation is done using a translation cache: the
    Translation Lookaside Buffers (TLB).
    \item
    There are one or more TLB into a single MMU: for example, we
    found two TLB on UltraSPARC architectures: one for instruction
    fetches and the other for data accesses. On IA-32 and MIPS
    architecture, there is only one mixed TLB.
    \item
    These caches are often \textbf{full associative} or high degree
    \textbf{set associative}, for performances reasons. But their size
    is very small:
    \begin{tabular}{|c|c|c|}
    \hline
    Microprocessor & TLB & Entries \\
    \hline
    Pentium (non-MMX) & Instruction & 32 entries, 4-way set associative \\
    \hline
    Pentium (non-MMX) & Data & 64 entries, 4-way set associative \\
    \hline
    Pentium 4 & Instruction & 128 entries, 4-way set associative \\
    \hline
    Pentium 4 & Data & 64 entries, full associative \\
    \hline
    UltraSPARC IIi & Instruction & 64 entries, full associative \\
    \hline
    UltraSPARC IIi & Data & 64 entries, full associative \\
    \hline
    MIPS R8000 & Mixed & 384 entries, 3-way set associative \\
    \hline
    \end{tabular}
  \end{itemize}
\end{frame}

% -)

\begin{frame}
  \frametitle{TLB Organization}

    \begin{itemize}
      \item A TLB is a cache where each entry is made of two parts :

      \begin{center}
        \pgfuseimage{detailed-tlb}
      \end{center}

      \item A cache-hit occurs when:
      \begin{enumerate}
      \item
        V = 1
      \item
        ID = current ASID \textbf{or} G = 1
      \item
        VA $\leq$ requested address $<$ VA + SZ
      \end{enumerate}
    \end{itemize}
\end{frame}

% -)

\begin{frame}
  \frametitle{Scenario of page-hit}

  \begin{center}
    \pgfuseimage{paging-step1}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Scenario of page-hit}

  \begin{center}
    \pgfuseimage{paging-step2}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Scenario of page-hit}

  \begin{center}
    \pgfuseimage{paging-step3}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Scenario of page-hit with error}

  \begin{center}
    \pgfuseimage{paging-error-step1}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Scenario of page-hit with error}

  \begin{center}
    \pgfuseimage{paging-error-step2}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Scenario of page-miss}

  \begin{center}
    \pgfuseimage{paging-miss-step1}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Scenario of page-miss}

  \begin{center}
    \pgfuseimage{paging-miss-step2}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{What to do on a TLB-miss?}

  When an error or TLB-miss occurs, an exception is thrown. There are
  three possible actions:

  \begin{itemize}
  \item
    Stopping the program and reporting the error to the user
    (\emph{Segmentation fault})
  \item
    Modifying the TLB entry to avoid the error (changing rights or
    ownership) and resuming the program
  \item
    Filling the missing entry and resuming the program
  \end{itemize}

  \-

  In the next slides, we will focus on the third point. This is not an
  error, remember that the TLB is a cache, meaning that it cannot host
  all the required entries.

\end{frame}

% -)

\begin{frame}
  \frametitle{Manual TLB entry replacement}

  Processors with very basic MMU let the kernel fill the TLB.

  \-

  On TLB-miss, an exception is thrown and the involved virtual address
  is passed through a register. The exception handler must look in its
  own structures an perform the TLB entry replacement by hand.

  \-

  A TLB replacement involves:

  \begin{itemize}
  \item
    Selecting an unlikely to be used entry to remove
  \item
    Building and adding the new entry
  \end{itemize}

\end{frame}

% -)

\begin{frame}
  \frametitle{Choosing a good entry to replace}

  Sometimes, the TLB cache is full, meaning that all of the few
  entries are used. Adding a new entry forces to remove an existing
  one.

  \-

  Some TLB implements a LRU (\emph{Least Recently Used}) algorithm and
  put in a register the number of the entry most unlikely to be used.

  \-

  Otherwise, the kernel can implement its own aging algorithms using
  the TLB entries facilities (accessed bit, user free
  bits\ldots). Another easier solution is to replace a random entry,
  this is fast but not really efficient.

\end{frame}

% -)

\begin{frame}
  \frametitle{MIPS example}

  \begin{center}
    \pgfuseimage{mips-tlb}
  \end{center}

  MIPS TLB entries are mapping two pages each, depending on the last
  bit of the virtual address (odd or even pages). Many page size are
  available, from 4 Kb to 256 Mb.

\end{frame}

% -)

\begin{frame}
  \frametitle{MIPS example}

  MMU registers list:

  \begin{itemize}
  \item
    ASID register: specified the current ASID value
  \item
    Index register: index of the TLB to use (for modifying or removing)
  \item
    Random register: random index of a TLB entry that can be replaced
    (the first dozen entries are protected)
  \item
    Tag register (called EntryHi): used to set the Tag word of an entry
  \item
    Entry registers (called EntryLo): used to set the Entry word of an entry
  \item
    PageMask register: used to specify the size of the page for the
    TLB entry
  \item
    Bad virtual address (BadVAddr) register: reports the address that
    caused page-miss of page-error
\end{itemize}

\end{frame}

% -)

\begin{frame}
  \frametitle{Exercise: tree-based replacement algorithm}

  Write a pseudo-algorithm for replacing TLB entries with a binary
  page tree.

  \-

  Your algorithm, given a virtual address, must fill the TLB with the
  correct entry or report an error if no translation can be done.

  \-

  Each bit of a virtual address correspond to a direction to take in
  the tree. A bit set tells to take the right child and a bit reset
  the left child.

  \-

  The leaf contains the translation. The page size is determined by
  the depth of a leaf.

\end{frame}

% -)

\begin{frame}
  \frametitle{Exercise: correction}

  \begin{enumerate}
  \item
    Pagesz $\leftarrow$ 31
  \item
    Get BadVAddr
  \item
    For each bit from left to right
    \begin{enumerate}
    \item
      Goto appropriate child
    \item
      In non-existent child, throw error
    \item
      On a leaf:
      \begin{itemize}
      \item
        Index $\leftarrow$ Random
      \item
        EntryHi $\leftarrow$ Build tag (Vaddr$_{31-pagesz}$, ASID)
      \item
        EntryLo $\leftarrow$ Build entry (Paddr, Writable)
      \item
        Pagemask $\leftarrow$ $2^{pagesz}-1$
      \end{itemize}
    \item
      Otherwise, Pagesz $\leftarrow$ Pagesz - 1
    \end{enumerate}
  \end{enumerate}

\end{frame}

% -)

\begin{frame}
  \frametitle{Semi-automatic TLB replacement}

  Some kind of MMU offers a semi-automatic TLB replacement mechanism.

  \-

  The principle of semi-automatic TLB management consist in a software
  cache in main memory. When no entry is found in the MMU's TLB, a
  preprogrammed replacement routine is called and search the software
  cache.

  \-

  If no entry is found in the software table, then an exception is
  raised to the kernel (we fallback to the previous scheme).

\end{frame}

% -)

\begin{frame}
  \frametitle{UltraSPARC example}

  On UltraSPARC, the software cache is called TSB (\emph{Translation
  Storage Buffer}) and is a direct-mapped cache of TTE
  (\emph{Translation Table Entry}). Searching the TSB involves only
  one memory access, this is quite acceptable.

  \-

  \begin{center}
    \pgfuseimage{direct-mapped}
  \end{center}

  The MMU is given only the base and the size of the TSB.

\end{frame}

% -)

\begin{frame}
  \frametitle{Full-automatic TLB replacement}

  Sophisticated MMU provides automatic TLB replacement algorithms,
  meaning that particular structures imposed by the MMU must be filled
  by the kernel.

  \-

  On TLB-miss, a preprogrammed handler is called and browse these
  structures to find a corresponding translation.

  \-

  As defining a big array for each translation should not be efficient
  (taking 4 Kb pages in a 4 Gb address space results in a 8 Mb array
  per address space), trees are often used for this purpose.

\end{frame}

% -)

\begin{frame}
  \frametitle{IA-32 example}

  \begin{center}
    \pgfuseimage{ia32-mmu}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{IA-32 example}

  \begin{center}
    \pgfuseimage{pd-pt}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Exercise: contiguous mapping with page-directory \&
  page-tables}

  Write a procedure that create a page-directory \& page-tables
  hierarchy for a precise memory mapping (given the virtual and
  physical addresses and the size of the mapping).

  \-

  The following operation are given:

  \begin{itemize}
  \item
    \textbf{PDE\_ENTRY}: get the page-directory entry corresponding to
    a virtual address
  \item
    \textbf{PTE\_ENTRY}: get the page-table entry corresponding to
    a virtual address
  \item
    \textbf{pd\_add\_table}: add a page-directory entry pointing to a
    page table
  \item
    \textbf{pd\_get\_table}: get a page-directory entry
  \item
    \textbf{pt\_add\_page}: add a page-table entry, mapping a precise
    virtual page to a physical one
  \end{itemize}

\end{frame}

% -)

\begin{frame}
  \frametitle{Exercise: correction}

  \begin{itemize}
  \item
    pde\_start $\leftarrow$ PDE\_ENTRY(Vaddr) \\
    pde\_end $\leftarrow$ PDE\_ENTRY(Vaddr + Size) \\
    pte\_start $\leftarrow$ PTE\_ENTRY(Vaddr) \\
    pte\_end $\leftarrow$ PTE\_ENTRY(Vaddr + Size)
  \item
    i $\leftarrow$ pde\_start to pde\_end
    \begin{itemize}
    \item
      if pd\_get\_table(i) not exist then pd\_add\_table(i, new table)
    \item
      table $\leftarrow$ pd\_get\_table(i)
    \item
      j $\leftarrow$ (pte\_start if i == pde\_start, 0 otherwise) to (pte\_end if i == pde\_end, 1024 otherwise)
      \begin{itemize}
      \item
        pt\_add\_page(table, j, translation)
      \end{itemize}
    \end{itemize}
  \end{itemize}

\end{frame}

% -)

\begin{frame}
  \frametitle{Cycles in virtual memory management}

  There are a few common cycles when dealing with virtual memory.

  \begin{itemize}
  \item
    When a TLB-miss handler is called, it may generate another
    TLB-miss when accessing the kernel's structures
  \item
    When filling page-directory \& page-tables to map a region of
    memory, the page directory and/or page-tables needs to be mapped
  \end{itemize}

  \-

  Sometimes, the CPU provides facilities to avoid this cycle: a region
  in virtual memory called a bypass region exists and permits to
  bypass the MMU (this means that putting the memory management
  structures in this area will never generate TLB-miss).

  \-

  Others CPU do not. So we have to show imagination.

\end{frame}

% -)

\begin{frame}
  \frametitle{MMU bypass on MIPS and UltraSPARC}

  Bypassing the MMU is easy on MIPS processors: a special segment of
  memory, called \emph{kseg0} and lying from 0x80000000 to 0x9FFFFFFF
  is mapping RAM from 0x0 to 0x1FFFFFFF (about 500 Mb). No MMU
  translations are needed.

  \-

  On UltraSPARC, special instruction are needed to bypass the
  MMU. These instructions are LDA and STA (and all variant with
  different data width). A special ASI (\emph{Address Space Index}) is
  passed and indicate the LSU to bypass MMU translation.

\end{frame}

% -)

\begin{frame}
  \frametitle{Mirroring on IA-32}

  Mirroring is a trick to avoid the need of mapping the page-directory
  and the page-table when accessing them.

  \-

  The principle of mirroring is to create a page-directory entry
  pointing to the page-directory itself. By doing this:

  \begin{itemize}
  \item
    Looking at MAKE\_ADDR(mirror, mirror, offset) refers to the
    page-directory itself
  \item
    Looking at MAKE\_ADDR(mirror, index\_pt, offset) refers to a
    page-table
  \end{itemize}

  \-

  This technique (thought unofficial) is often used to solve our problem.

\end{frame}

% -)

\begin{frame}
  \frametitle{TLB coherency}

  The TLB is a cache, meaning that it represent a large amount of data
  placed into a smaller room.

  \-

  The algorithms described above describe how to maintain internal
  structures used by the TLB. But modifiying these structures does not
  affect the TLB directly.

  \-

  Let's imagine a page translation -- for example a page-table entry
  on IA-32 -- already in the TLB. Il you change or remove this
  page-table entry in main memory, the TLB still have the
  translation. This is a cache coherency error.

\end{frame}

% -)

\begin{frame}
  \frametitle{TLB incoherency example}

  \begin{center}
    \pgfuseimage{tlb-coherency-step1}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{TLB incoherency example}

  \begin{center}
    \pgfuseimage{tlb-coherency-step2}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{TLB invalidation or flushing}

  To avoid the previous problem, TLB entries can be invalidated to
  force their content to be reloaded from kernel's virtual mapping
  structures.

  \-

  TLB flushing involves invalidating all the TLB entries. Switching
  address space on IA-32 (loading a new value in PDBR) forces a flush
  of TLB.

  \-

  But invalidating the TLB should be avoided because next accesses to the
  TLB will be much more longer (TLB-miss, TLB replacement algorithm\ldots).

\end{frame}

% -)

\begin{frame}
  \frametitle{Coherency on multiprocessor platform}

  \begin{center}
    \pgfuseimage{smp-coherency-step1}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Coherency on multiprocessor platform}

  \begin{center}
    \pgfuseimage{smp-coherency-step2}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Coherency on multiprocessor platform}

  \begin{center}
    \pgfuseimage{smp-coherency-step3}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Unmapping}

  Unmapping memory pages is very easy:

  \begin{itemize}
  \item
    Remove the translation into the kernel's or MMU's structures
  \item
    Do not forget to invalidate the corresponding entry in TLB if
    needed
  \end{itemize}

\end{frame}

% -)

\begin{frame}
  \frametitle{Using paging with segmentation}

  Segmentation is often used a bit with paging.

  \-

  On MIPS for example, there is no bit in TLB entries to tell if the
  page belong to the kernel or to the user. So this protection between
  privilege level is done by using the segmentation, putting the user
  pages below the 2 Gb and the kernel pages above.

  \-

  On IA-32, there is no way to tell if a page is code or data (this
  feature is added by AMD with the NX bit). For this reason, using a
  flat model for segmentation is not sure, since the stack can be
  executed for example. Fixing this problem can be done by forcing the
  task to be in a non executable segment.

\end{frame}

%
% allocation algorithms
%

\section{Allocation algorithms}

% -)

\begin{frame}
  \frametitle{Allocator front-end and back-end}

  An allocator provides a front-end (\emph{malloc}, \emph{realloc},
  \emph{free}) and uses a back-end (\emph{brk}, \emph{sbrk},
  \emph{mmap}\ldots).

  \-

  The goal of a memory allocator is to provide fine-grain memory
  allocation and deallocation routines (opposed to large-grain
  functions like \emph{mmap}).

  \-

  Problems encountered in memory allocation:

  \begin{itemize}
  \item
    \textbf{Speed}, as memory allocation and freeing are largely used
    in programs
  \item
    \textbf{Fragmentation}, an efficient allocator should not waste
    space
  \item
    \textbf{Concurrent operations} (or scalability), because
    multiprocessor systems are designed for high-performances
  \item
    \textbf{Cache placement}, a good allocator must take care of cache
    line size and cache placement, even on multiprocessor (non-shared
    caches)
  \end{itemize}

\end{frame}

% -)

\begin{frame}
  \frametitle{Allocating physical pages rather than memory chunks}

  In the next few slides, we will present four allocation techniques.

  \-

  The algorithms described are suitable for fine-grain allocators, but
  can also be used for page allocation.

  \-

  In the case of page allocation, the sizes are then represented in
  page numbers instead of bytes, and the initial space provided for
  allocation is the entire or partial RAM.

\end{frame}

% -)

\begin{frame}
  \frametitle{Free-list based allocation}

  XXX

\end{frame}

% -)

\begin{frame}
  \frametitle{Free-list based allocation -- comparisons}

  XXX

  \begin{itemize}
  \item
    First-fit
  \item
    Best-fit
  \item
    Worst-fit
  \end{itemize}

\end{frame}

% -)

\begin{frame}
  \frametitle{Buddy systems}

  XXX

\end{frame}

% -)

\begin{frame}
  \frametitle{Buddy systems -- example}

  XXX

\end{frame}

% -)

\begin{frame}
  \frametitle{Buddy systems -- Fibonacci rather than power-of-two}

  XXX

\end{frame}

% -)

\begin{frame}
  \frametitle{Slab allocation}

  SunOS 5.4 introduced a new memory allocator called the slab
  allocator. This allocator improved performances in time from 5 to 15
  \% and in space (fragmentation) for about 30 \%.

  \-

  The slab allocator was next implemented in Linux, FreeBSD, and a lot
  of other operating systems.

  \-

  The principles behind this algorithm is simple:

  \begin{itemize}
  \item
    The user allocates memory for objects, so it is likely that many
    objects have the same size
  \item
    When an object is freed, reclaiming memory directly is premature,
    behaving as a cache is a better behavior
  \item
    Managing the allocator's internal structures must not be more
    expensive than allocating the memory itself
  \end{itemize}

\end{frame}

% -)

\begin{frame}
  \frametitle{Slab allocation -- slabs}

  A slab is a contiguous space carved in multiple chunks of same size.

  \begin{center}
    \pgfuseimage{slabs}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Slab allocation -- large objects}

  Large objects are not handled in slabs. They are directly allocated
  using \emph{mmap} or equivalent.

\end{frame}

% -)

\begin{frame}
  \frametitle{Slab allocation -- construction and destruction}

  When using object caches (like in a slab allocator), it is possible
  to provide constructor and destructors for objects.

  \-

  Thus, the cost of initializing and destroying by hand objects is
  reduced since the slab allocator knows when it is necessary to
  construct/destruct an object.

  \-

  This principle is not used for user allocation since the allocator
  does not know the nature of the objects, but it is heavily used in
  the allocator itself. For example, slab and buffer descriptors lean
  on an object cache using constructor and destructors so it is never
  necessary to initialize or uninitialize objects (leading to better
  overall performances of the allocator).

\end{frame}

% -)

\begin{frame}
  \frametitle{Slab allocation -- example}

  \begin{center}
    \pgfuseimage{slab-allocator}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Hoard memory allocator}

  Hoard is a young memory allocator, usable on many operating systems
  (Windows, Linux, Solaris) as a DLL or preloadable library.

  \-

  Hoard aims multiprocessor systems and multithreaded applications.

  \-

  Hoard proposes low fragmentation rates, linear scale in
  multiprocessor systems and false-sharing avoidance.

\end{frame}

% -)

\begin{frame}
  \frametitle{Hoard memory allocator -- principle}

  XXX
\end{frame}

% -)

\begin{frame}
  \frametitle{Hoard memory allocator -- example}

  XXX
\end{frame}

% -)

\begin{frame}
  \frametitle{Other tips \& tricks}

  The next slides present a few techniques used to optimize memory
  allocation.

\end{frame}

% -)

\begin{frame}
  \frametitle{Stack expansion}

  \begin{center}
    \pgfuseimage{stack-exp-step1}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Stack expansion}

  \begin{center}
    \pgfuseimage{stack-exp-step2}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Stack expansion}

  \begin{center}
    \pgfuseimage{stack-exp-step3}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Allocation-on-demand}

  Allocation-on-demand consists in reserving a region of virtual
  memory without allocating the page frames.

  \-

  When a page-miss occurs, a page frame is then allocated and the
  virtual space corresponding to the miss is updated.

  \-

  This technique allows to delay the allocation of page-frame, and
  sometime to avoid allocating pages for unused chunks of memory.

\end{frame}

% -)

\begin{frame}
  \frametitle{Allocation-on-demand}

  \begin{center}
    \pgfuseimage{alloc-demand-step1}
  \end{center}

\end{frame}


% -)

\begin{frame}
  \frametitle{Allocation-on-demand}

  \begin{center}
    \pgfuseimage{alloc-demand-step2}
  \end{center}

\end{frame}


% -)

\begin{frame}
  \frametitle{Allocation-on-demand}

  \begin{center}
    \pgfuseimage{alloc-demand-step3}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Copy-on-write}

  Copy-on-write method (or COW) is often used when cloning and address
  space (eg: fork).

  \-

  This mechanism uses read-only markers on TLB entries and reference
  counters on page frames.

\end{frame}

% -)

\begin{frame}
  \frametitle{Copy-on-write}

  \begin{center}
    \pgfuseimage{cow-step1}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Copy-on-write}

  \begin{center}
    \pgfuseimage{cow-step2}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Copy-on-write}

  \begin{center}
    \pgfuseimage{cow-step3}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Copy-on-write}

  \begin{center}
    \pgfuseimage{cow-step4}
  \end{center}

\end{frame}

% -)

\begin{frame}
  \frametitle{Out of memory (OOM)}

  When the system is completely out-of-memory (physical memory and
  swap), there are only a few option:

  \begin{itemize}
  \item
    Do nothing. Memory allocation routine fails. Programs should be
    able to handle those cases (\emph{malloc} returns NULL\ldots)
  \item
    Try to release memory
  \end{itemize}

  \-

  The first solution should be the best, but in fact only a few
  programs manage out-of-memory cases and crash. In addition, what
  happen if the kernel needs memory?

\end{frame}

% -)

\begin{frame}
  \frametitle{Linux OOM-killer}

  Under Linux, when the system has no more memory to allocate, a
  process called the OOM-killer is called to release memory.

  \-

  Before acting, the OOM-killer go through the following checklist:

  \begin{itemize}
  \item
    Has it been more than 5 seconds since the last failure?
  \item
    Have we failed within the last second?
  \item
    Did we failed 10 times at least in the last 5 seconds?
  \item
    Has a process been killed within the last 5 seconds?
  \end{itemize}

  \-

  These conditions must be checked because sometime we can leave OOM
  state without killing a process (for example, a program is waiting
  for an I/O to finish and this will free a few pages).

  \-

  The OOM-killer choose a process to kill, using the following
  formula:

  $$badness\_for\_task = \frac{total\_vm\_for\_task}
  {\sqrt(cpu\_time\_in\_seconds) * \sqrt[4](cpu\_time\_in\_minutes)}$$

\end{frame}

% -)

\begin{frame}
  \frametitle{Summary of page-miss cases}

  \begin{itemize}
  \item
  Region valid, page frame not allocated\\
  $\rightarrow$ Allocate the page frame and resume.
  \item
  Region valid, marked read-only\\
  $\rightarrow$ Check for a copy-on-write page. Allocate a new page
  frame, copy the data and resume. Otherwise, throw error.
  \item
  Region invalid, but below an expandable region\\
  $\rightarrow$ Expand the region, allocate a page and resume.
  \item
  Region valid, page in swap space\\
  $\rightarrow$ Remove another page from memory, load the required
  page from storage and resume.
  \item
  Region invalid or access denied\\
  $\rightarrow$ Throw an error. If this happens in the kernel address
  space, it is a kernel bug, enter in panic mode.
  \end{itemize}

\end{frame}

%
% swapping
%

\section{Swapping}

% -)

\begin{frame}
  \frametitle{Principles}

  XXX
\end{frame}

% -)

\begin{frame}
  \frametitle{Page replacement algorithm}

  XXX FIFO, NRU, LRU, NFU, Aging
\end{frame}

% -)

\begin{frame}
  \frametitle{Implementation on IA-32}

  XXX
\end{frame}

% -)

\begin{frame}
  \frametitle{Swapping under Linux}

  XXX
\end{frame}

%
% bibliography
%

\section{Bibliography}

\begin{thebibliography}{4}

%  \bibitem{ID}
%    Sample
%    \newblock Sample

\end{thebibliography}

\end{document}
